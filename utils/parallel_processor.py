#!/usr/bin/env python3
"""
Parallel Processor - –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ data
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Optional, Any, Callable, Tuple
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
import time
from pathlib import Path

logger = logging.getLogger(__name__)

class ParallelProcessor:
    """–ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ data"""
    
    def __init__(self, config: Dict):
        self.max_workers = config.get('parallel', {}).get('max_workers', mp.cpu_count())
        self.use_processes = config.get('parallel', {}).get('use_processes', True)
        self.timeout = config.get('parallel', {}).get('timeout', 300)
        self.chunk_size = config.get('parallel', {}).get('chunk_size', 100)
        
    def process_tickers_parallel(self, 
                               tickers: List[str], 
                               timeframes: List[str], 
                               func: Callable,
                               **kwargs) -> List[Any]:
        """
        –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Ç—ñ–∫–µ—Ä—ñ–≤
        
        Args:
            tickers: –°–ø–∏—Å–æ–∫ —Ç—ñ–∫–µ—Ä—ñ–≤
            timeframes: –°–ø–∏—Å–æ–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ñ–≤
            func: –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            **kwargs: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        
        Returns:
            List[Any]: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ–±—Ä–æ–±–∫–∏
        """
        logger.info(f"[RESTART] Processing {len(tickers)} x {len(timeframes)} tasks in parallel")
        
        # [TARGET] –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–¥–∞–Ω–Ω—è
        tasks = []
        for ticker in tickers:
            for timeframe in timeframes:
                tasks.append((ticker, timeframe))
        
        logger.info(f"[DATA] Total tasks: {len(tasks)}, workers: {self.max_workers}, processes: {self.use_processes}")
        
        # [TARGET] –í–∏–±—ñ—Ä –≤–∏–∫–æ–Ω–∞–≤—Ü—è
        executor_class = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor
        
        results = []
        start_time = time.time()
        
        with executor_class(max_workers=self.max_workers) as executor:
            # [TARGET] –ó–∞–ø—É—Å–∫–∞—î–º–æ –∑–∞–≤–¥–∞–Ω–Ω—è
            future_to_task = {
                executor.submit(self._process_single_task, func, task, **kwargs): task
                for task in tasks
            }
            
            # [TARGET] –ó–±–∏—Ä–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            completed = 0
            for future in as_completed(future_to_task, timeout=self.timeout):
                task = future_to_task[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1
                    
                    if completed % 10 == 0:
                        elapsed = time.time() - start_time
                        logger.info(f"[DATA] Completed {completed}/{len(tasks)} tasks in {elapsed:.1f}s")
                        
                except Exception as e:
                    logger.error(f"[ERROR] Task {task} failed: {e}")
                    results.append(None)
        
        elapsed = time.time() - start_time
        successful = sum(1 for r in results if r is not None)
        
        logger.info(f"[OK] Parallel processing completed: {successful}/{len(tasks)} successful in {elapsed:.1f}s")
        
        return results
    
    def _process_single_task(self, func: Callable, task: Tuple[str, str], **kwargs) -> Any:
        """–û–±—Ä–æ–±–∫–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è"""
        ticker, timeframe = task
        try:
            return func(ticker, timeframe, **kwargs)
        except Exception as e:
            logger.error(f"[ERROR] Error processing {ticker}_{timeframe}: {e}")
            raise
    
    def process_dataframe_chunks(self, 
                                 df: pd.DataFrame, 
                                 func: Callable,
                                 chunk_size: Optional[int] = None,
                                 **kwargs) -> pd.DataFrame:
        """
        –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ DataFrame —á–∞–Ω–∫–∞–º–∏
        
        Args:
            df: DataFrame –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            func: –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            chunk_size: –†–æ–∑–º—ñ—Ä —á–∞–Ω–∫—É
            **kwargs: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        
        Returns:
            pd.DataFrame: –û–±'—î–¥–Ω–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        """
        if chunk_size is None:
            chunk_size = self.chunk_size
        
        logger.info(f"[RESTART] Processing DataFrame in parallel chunks: {len(df)} rows, chunk_size: {chunk_size}")
        
        # [TARGET] –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —á–∞–Ω–∫–∏
        chunks = []
        for i in range(0, len(df), chunk_size):
            chunk = df.iloc[i:i+chunk_size].copy()
            chunks.append(chunk)
        
        logger.info(f"[DATA] Created {len(chunks)} chunks")
        
        # [TARGET] –û–±—Ä–æ–±–ª—è—î–º–æ —á–∞–Ω–∫–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
        executor_class = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor
        
        results = []
        start_time = time.time()
        
        with executor_class(max_workers=self.max_workers) as executor:
            future_to_chunk = {
                executor.submit(func, chunk, **kwargs): i
                for i, chunk in enumerate(chunks)
            }
            
            completed = 0
            for future in as_completed(future_to_chunk):
                chunk_idx = future_to_chunk[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1
                    
                    if completed % 5 == 0:
                        elapsed = time.time() - start_time
                        logger.info(f"[DATA] Completed {completed}/{len(chunks)} chunks in {elapsed:.1f}s")
                        
                except Exception as e:
                    logger.error(f"[ERROR] Chunk {chunk_idx} failed: {e}")
                    results.append(pd.DataFrame())
        
        # [TARGET] –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        if results:
            logger.info("[RESTART] Concatenating chunk results")
            final_result = pd.concat(results, ignore_index=True)
            
            # [TARGET] –°–æ—Ä—Ç—É—î–º–æ —è–∫—â–æ —î —ñ–Ω–¥–µ–∫—Å
            if 'date' in final_result.columns:
                final_result = final_result.sort_values('date')
            
            return final_result
        else:
            return pd.DataFrame()
    
    def benchmark_performance(self, 
                            test_func: Callable,
                            test_data: List,
                            **kwargs) -> Dict[str, Any]:
        """
        –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        
        Args:
            test_func: –¢–µ—Å—Ç–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è
            test_data: –¢–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ
            **kwargs: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        
        Returns:
            Dict[str, Any]: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫—É
        """
        logger.info("üèÉ Running performance benchmark...")
        
        # [TARGET] –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞
        start_time = time.time()
        sequential_results = []
        for item in test_data:
            result = test_func(item, **kwargs)
            sequential_results.append(result)
        sequential_time = time.time() - start_time
        
        # [TARGET] –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞
        start_time = time.time()
        parallel_results = self.process_tickers_parallel(
            test_data, [], lambda x, y: test_func(x), **kwargs
        )
        parallel_time = time.time() - start_time
        
        # [TARGET] –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫
        speedup = sequential_time / parallel_time if parallel_time > 0 else 0
        efficiency = speedup / self.max_workers * 100
        
        benchmark_results = {
            'sequential_time': sequential_time,
            'parallel_time': parallel_time,
            'speedup': speedup,
            'efficiency_percent': efficiency,
            'workers': self.max_workers,
            'use_processes': self.use_processes,
            'items_processed': len(test_data),
            'items_per_second': len(test_data) / parallel_time
        }
        
        logger.info(f"[DATA] Benchmark results:")
        logger.info(f"   Sequential: {sequential_time:.2f}s")
        logger.info(f"   Parallel: {parallel_time:.2f}s")
        logger.info(f"   Speedup: {speedup:.2f}x")
        logger.info(f"   Efficiency: {efficiency:.1f}%")
        
        return benchmark_results
    
    def suggest_optimal_config(self, data_size: int, task_complexity: str = 'medium') -> Dict[str, Any]:
        """
        –ü—ñ–¥–∫–∞–∑–∞—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
        
        Args:
            data_size: –†–æ–∑–º—ñ—Ä data
            task_complexity: –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –∑–∞–≤–¥–∞–Ω–Ω—è ('low', 'medium', 'high')
        
        Returns:
            Dict[str, Any]: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        """
        recommendations = {}
        
        # [TARGET] –ö—ñ–ª—å–∫—ñ—Å—Ç—å workers
        if task_complexity == 'low':
            recommendations['max_workers'] = min(mp.cpu_count(), 8)
        elif task_complexity == 'medium':
            recommendations['max_workers'] = min(mp.cpu_count(), 4)
        else:  # high
            recommendations['max_workers'] = min(mp.cpu_count(), 2)
        
        # [TARGET] Processes vs Threads
        if task_complexity == 'high':
            recommendations['use_processes'] = True
        else:
            recommendations['use_processes'] = False
        
        # [TARGET] Chunk size
        if data_size < 1000:
            recommendations['chunk_size'] = 100
        elif data_size < 10000:
            recommendations['chunk_size'] = 500
        else:
            recommendations['chunk_size'] = 1000
        
        # [TARGET] Timeout
        if task_complexity == 'high':
            recommendations['timeout'] = 600
        else:
            recommendations['timeout'] = 300
        
        logger.info(f"[DATA] Recommended config for {task_complexity} complexity, {data_size} items:")
        for key, value in recommendations.items():
            logger.info(f"   {key}: {value}")
        
        return recommendations


if __name__ == "__main__":
    print("Parallel Processor - –≥–æ—Ç–æ–≤–∏–π –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è")
    print("[RESTART] –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ data –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é")
