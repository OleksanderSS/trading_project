# core/stages/stage_manager.py - –ú–µnot–¥–∂–µ—Ä —Ä–æwith–ø–æ–¥and–ª—É –µand–øand–≤ with –∫–µ—à—É–≤–∞–Ω–Ω—è–º

import os
import json
import pickle
import hashlib
from datetime import datetime
from typing import Dict, Any, Optional, Tuple, List
import pandas as pd
import logging

# –í–ò–ü–†–ê–í–õ–ï–ù–û: –≤–∏–¥–∞–ª—è—î–º–æ —ñ–º–ø–æ—Ä—Ç –≤–∏–¥–∞–ª–µ–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó
# from core.stages.stage_1_collectors_layer import run_stage_1_collect
from core.stages.stage_2_enrichment import run_stage_2_enrich_optimized, run_stage_2_enrichment_fixed
from core.stages.stage_3_features import prepare_stage3_datasets
from core.stages.stage_4_benchmark import benchmark_all_models
from config.config_loader import load_yaml_config
from utils.trading_calendar import TradingCalendar
from utils.colab_utils import colab_utils
from utils.logger import ProjectLogger

logger = ProjectLogger.get_logger("TradingProjectLogger")

class StageManager:
    """–ú–µnot–¥–∂–µ—Ä –µand–øand–≤ with –∫–µ—à—É–≤–∞–Ω–Ω—è–º and —Ä–æwith–ø–æ–¥and–ª–æ–º"""
    
    def __init__(self, base_path: str = "data/cache/stages"):
        self.base_path = base_path
        self.ensure_directories()
        
    def ensure_directories(self):
        """–°—Ç–≤–æ—Ä—é—î –¥–∏—Ä–µ–∫—Ç–æ—Äand—ó for –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        dirs = [
            "stage1_raw",
            "stage2_enriched", 
            "stage3_features",
            "stage4_models",
            "stage5_signals"
        ]
        for dir_name in dirs:
            os.makedirs(os.path.join(self.base_path, dir_name), exist_ok=True)
    
    def get_cache_path(self, stage: str, params_hash: str) -> str:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ —à–ª—è—Ö –¥–æ –∫–µ—à—É"""
        return os.path.join(self.base_path, stage, f"cache_{params_hash}.pkl")
    
    def get_params_hash(self, params: Dict[str, Any]) -> str:
        """–ì–µnot—Ä—É—î —Ö–µ—à –ø–∞—Ä–∞–º–µ—Ç—Äand–≤ for –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        params_str = json.dumps(params, sort_keys=True, default=str)
        return hashlib.md5(params_str.encode()).hexdigest()[:16]
    
    def is_cache_valid(self, cache_path: str, max_age_hours: int = 24) -> bool:
        """–ü–µ—Ä–µ–≤and—Ä—è—î —á–∏ –¥and–π—Å–Ω–∏–π –∫–µ—à"""
        if not os.path.exists(cache_path):
            return False
        
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        age = datetime.now() - file_time
        return age.total_seconds() < max_age_hours * 3600
    
    def save_cache(self, cache_path: str, data: Any):
        """–ó–±–µ—Äand–≥–∞—î –¥–∞–Ωand –≤ –∫–µ—à"""
        import os
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        with open(cache_path, 'wb') as f:
            pickle.dump(data, f)
        logger.info(f"[StageManager] –ó–±–µ—Ä–µ–∂–µ–Ω–æ –∫–µ—à: {cache_path}")
    
    def load_cache(self, cache_path: str) -> Any:
        """–ó–∞–≤–∞–Ωand–∂—É—î –¥–∞–Ωand with –∫–µ—à—É"""
        with open(cache_path, 'rb') as f:
            data = pickle.load(f)
        logger.info(f"[StageManager] –ó–∞–≤–∞–Ωand–∂–µ–Ω–æ –∫–µ—à: {cache_path}")
        return data
    
    def run_stage_1(self, debug_no_network: bool = False, force_refresh: bool = False) -> Dict[str, Any]:
        """–ï—Ç–∞–ø 1: –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö"""
        # Import TICKERS and TIME_FRAMES for cache parameters
        from config.config import TICKERS, TIME_FRAMES
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –æ–±—á–∏—Å–ª—é—î–º–æ cache_path –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        params = {"tickers": list(TICKERS.keys()), "timeframes": TIME_FRAMES}
        params_hash = self.get_params_hash(params)
        cache_path = self.get_cache_path("stage1_raw", params_hash)
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É - —Ä—ñ–∑–Ω—ñ TTL –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö
        if not force_refresh and os.path.exists(cache_path):
            try:
                cached_data = self.load_cache(cache_path)
                
                if cached_data and len(cached_data) > 0:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∞—Å –∫–µ—à—É
                    cache_time = cached_data.get('_metadata', {}).get('last_update_time')
                    if cache_time:
                        from datetime import datetime, timedelta
                        cache_age = datetime.now() - cache_time
                        
                        # –†—ñ–∑–Ω—ñ TTL –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö
                        all_fresh = True
                        data_summary = {}
                        
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–µ–Ω —Ç–∏–ø –¥–∞–Ω–∏—Ö –æ–∫—Ä–µ–º–æ
                        for key, value in cached_data.items():
                            if key.startswith('_'):  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
                                continue
                            
                            # –†—ñ–∑–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—ñ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ç–∞–π–º—Ñ—Ä–µ–π–º—É
                            ttl_hours = 2  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
                            
                            if key in ['prices', 'prices_by_timeframe']:
                                # –†–Ü–ó–ù–ê –ß–ê–°–¢–û–¢–ê –û–ù–û–í–õ–ï–ù–ù–Ø –î–õ–Ø –†–Ü–ó–ù–ò–• –¢–ê–ô–ú–§–†–ï–ô–ú–Ü–í
                                if isinstance(value, dict):  # prices_by_timeframe
                                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–µ–Ω —Ç–∞–π–º—Ñ—Ä–µ–π–º –æ–∫—Ä–µ–º–æ
                                    timeframes_ttl = {
                                        '5m': 0.17,    # 10 —Ö–≤–∏–ª–∏–Ω –¥–ª—è 5-—Ö–≤–∏–ª–∏–Ω–Ω–∏—Ö –¥–∞–Ω–∏—Ö
                                        '15m': 0.5,    # 30 —Ö–≤–∏–ª–∏–Ω –¥–ª—è 15-—Ö–≤–∏–ª–∏–Ω–Ω–∏—Ö  
                                        '60m': 1.0,    # 1 –≥–æ–¥–∏–Ω–∞ –¥–ª—è 60-—Ö–≤–∏–ª–∏–Ω–Ω–∏—Ö
                                        '1d': 12.0     # 12 –≥–æ–¥–∏–Ω –¥–ª—è –¥–µ–Ω–Ω–∏—Ö (—Ä–∞–∑ –Ω–∞ –¥–µ–Ω—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ)
                                    }
                                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–∫–æ—Ä–æ—Ç—à–∏–π TTL –¥–ª—è –≤—Å—å–æ–≥–æ —Å–ª–æ–≤–Ω–∏–∫–∞
                                    ttl_hours = min(timeframes_ttl.values())
                                    logger.info(f"[Stage1] Using timeframe-specific TTL: {timeframes_ttl}")
                                else:
                                    ttl_hours = 0.5  # 30 —Ö–≤–∏–ª–∏–Ω –¥–ª—è –∑–≤–∏—á–∞–π–Ω–∏—Ö —Ü—ñ–Ω
                            elif key in ['news', 'newsapi', 'rss', 'google_news']:
                                ttl_hours = 4  # 4 –≥–æ–¥–∏–Ω–∏ –¥–ª—è –Ω–æ–≤–∏–Ω - –∞–∫—Ç—É–∞–ª—å–Ω—ñ –Ω–æ–≤–∏–Ω–∏ –≤–∞–∂–ª–∏–≤—ñ
                            elif key in ['fred']:
                                ttl_hours = 6  # 6 –≥–æ–¥–∏–Ω –¥–ª—è –º–∞–∫—Ä–æ –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤
                            elif key in ['crypto_prices']:
                                ttl_hours = 1  # 1 –≥–æ–¥–∏–Ω–∞ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
                            elif key in ['google_trends']:
                                ttl_hours = 12  # 12 –≥–æ–¥–∏–Ω –¥–ª—è —Ç—Ä–µ–Ω–¥—ñ–≤
                            else:
                                ttl_hours = 2  # 2 –≥–æ–¥–∏–Ω–∏ –¥–ª—è —ñ–Ω—à–∏—Ö –¥–∞–Ω–∏—Ö
                            
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –¥–∞–Ω—ñ —Å–≤—ñ–∂—ñ –¥–ª—è —Ü—å–æ–≥–æ —Ç–∏–ø—É
                            if cache_age.total_seconds() < ttl_hours * 3600:
                                if hasattr(value, 'shape') and value.shape[0] > 0:
                                    data_summary[key] = f"‚úÖ {value.shape} (—Å–≤—ñ–∂–∏–π)"
                                elif isinstance(value, dict) and len(value) > 0:
                                    data_summary[key] = f"‚úÖ dict with {len(value)} items (—Å–≤—ñ–∂–∏–π)"
                                elif isinstance(value, list) and len(value) > 0:
                                    data_summary[key] = f"‚úÖ list with {len(value)} items (—Å–≤—ñ–∂–∏–π)"
                            else:
                                all_fresh = False  # –Ø–∫—â–æ —Ö–æ—á–∞ –± –æ–¥–∏–Ω –∑–∞—Å—Ç–∞—Ä—ñ–≤ - –æ–Ω–æ–≤–ª—é—î–º–æ –≤—Å–µ
                                if hasattr(value, 'shape') and value.shape[0] > 0:
                                    data_summary[key] = f"‚è∞ {value.shape} (–∑–∞—Å—Ç–∞—Ä—ñ–≤ –Ω–∞ {cache_age.total_seconds()/3600:.1f}–≥)"
                                elif isinstance(value, dict) and len(value) > 0:
                                    data_summary[key] = f"‚è∞ dict with {len(value)} items (–∑–∞—Å—Ç–∞—Ä—ñ–≤ –Ω–∞ {cache_age.total_seconds()/3600:.1f}–≥)"
                                elif isinstance(value, list) and len(value) > 0:
                                    data_summary[key] = f"‚è∞ list with {len(value)} items (–∑–∞—Å—Ç–∞—Ä—ñ–≤ –Ω–∞ {cache_age.total_seconds()/3600:.1f}–≥)"
                        
                        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –Ü–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è - —Ç—ñ–ª—å–∫–∏ –∑–∞—Å—Ç–∞—Ä—ñ–ª—ñ –¥–∞–Ω—ñ
                        stale_data_types = []
                        fresh_data_types = []
                        
                        for key, value in cached_data.items():
                            if key.startswith('_'):  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
                                continue
                            
                            # –†—ñ–∑–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—ñ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ç–∞–π–º—Ñ—Ä–µ–π–º—É
                            ttl_hours = 2  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
                            
                            if key in ['prices', 'prices_by_timeframe']:
                                # –†–Ü–ó–ù–ê –ß–ê–°–¢–û–¢–ê –û–ù–û–í–õ–ï–ù–ù–Ø –î–õ–Ø –†–Ü–ó–ù–ò–• –¢–ê–ô–ú–§–†–ï–ô–ú–Ü–í
                                if isinstance(value, dict):  # prices_by_timeframe
                                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–µ–Ω —Ç–∞–π–º—Ñ—Ä–µ–π–º –æ–∫—Ä–µ–º–æ
                                    timeframes_ttl = {
                                        '5m': 0.17,    # 10 —Ö–≤–∏–ª–∏–Ω –¥–ª—è 5-—Ö–≤–∏–ª–∏–Ω–Ω–∏—Ö –¥–∞–Ω–∏—Ö
                                        '15m': 0.5,    # 30 —Ö–≤–∏–ª–∏–Ω –¥–ª—è 15-—Ö–≤–∏–ª–∏–Ω–Ω–∏—Ö  
                                        '60m': 1.0,    # 1 –≥–æ–¥–∏–Ω–∞ –¥–ª—è 60-—Ö–≤–∏–ª–∏–Ω–Ω–∏—Ö
                                        '1d': 12.0     # 12 –≥–æ–¥–∏–Ω –¥–ª—è –¥–µ–Ω–Ω–∏—Ö (—Ä–∞–∑ –Ω–∞ –¥–µ–Ω—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ)
                                    }
                                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–∫–æ—Ä–æ—Ç—à–∏–π TTL –¥–ª—è –≤—Å—å–æ–≥–æ —Å–ª–æ–≤–Ω–∏–∫–∞
                                    ttl_hours = min(timeframes_ttl.values())
                                    logger.info(f"[Stage1] Using timeframe-specific TTL: {timeframes_ttl}")
                                else:
                                    ttl_hours = 0.5  # 30 —Ö–≤–∏–ª–∏–Ω –¥–ª—è –∑–≤–∏—á–∞–π–Ω–∏—Ö —Ü—ñ–Ω
                            elif key in ['news', 'newsapi', 'rss', 'google_news']:
                                ttl_hours = 4  # 4 –≥–æ–¥–∏–Ω–∏ –¥–ª—è –Ω–æ–≤–∏–Ω - –∞–∫—Ç—É–∞–ª—å–Ω—ñ –Ω–æ–≤–∏–Ω–∏ –≤–∞–∂–ª–∏–≤—ñ
                            elif key in ['fred']:
                                ttl_hours = 6  # 6 –≥–æ–¥–∏–Ω –¥–ª—è –º–∞–∫—Ä–æ –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤
                            elif key in ['crypto_prices']:
                                ttl_hours = 1  # 1 –≥–æ–¥–∏–Ω–∞ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
                            elif key in ['google_trends']:
                                ttl_hours = 12  # 12 –≥–æ–¥–∏–Ω –¥–ª—è —Ç—Ä–µ–Ω–¥—ñ–≤
                            else:
                                ttl_hours = 2  # 2 –≥–æ–¥–∏–Ω–∏ –¥–ª—è —ñ–Ω—à–∏—Ö –¥–∞–Ω–∏—Ö
                            
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –¥–∞–Ω—ñ —Å–≤—ñ–∂—ñ –¥–ª—è —Ü—å–æ–≥–æ —Ç–∏–ø—É
                            if cache_age.total_seconds() < ttl_hours * 3600:
                                if hasattr(value, 'shape') and value.shape[0] > 0:
                                    fresh_data_types.append(f"{key}: {value.shape}")
                                elif isinstance(value, dict) and len(value) > 0:
                                    fresh_data_types.append(f"{key}: dict with {len(value)} items")
                                elif isinstance(value, list) and len(value) > 0:
                                    fresh_data_types.append(f"{key}: list with {len(value)} items")
                            else:
                                stale_data_types.append(key)
                        
                        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –Ø–∫—â–æ —î —Å–≤—ñ–∂—ñ –¥–∞–Ω—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ó—Ö + –æ–Ω–æ–≤–ª—é—î–º–æ –∑–∞—Å—Ç–∞—Ä—ñ–ª—ñ
                        if fresh_data_types and not stale_data_types:
                            logger.info(f"[StageManager] ‚úÖ –í—Å—ñ –¥–∞–Ω—ñ —Å–≤—ñ–∂—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–µ—à")
                            for item in fresh_data_types:
                                logger.info(f"  ‚úÖ {item}")
                            return cached_data
                        elif fresh_data_types and stale_data_types:
                            logger.info(f"[StageManager] üîÑ –ß–∞—Å—Ç–∫–æ–≤–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è: —Å–≤—ñ–∂—ñ {len(fresh_data_types)}, –∑–∞—Å—Ç–∞—Ä—ñ–ª—ñ {len(stale_data_types)}")
                            for item in fresh_data_types:
                                logger.info(f"  ‚úÖ {item}")
                            for item in stale_data_types:
                                logger.info(f"  üîÑ {item}")
                            # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ —á–∞—Å—Ç–∫–æ–≤–∏–º –æ–Ω–æ–≤–ª–µ–Ω–Ω—è–º
                        else:
                            logger.info(f"[StageManager] üîÑ –ü–æ–≤–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è - –≤—Å—ñ –¥–∞–Ω—ñ –∑–∞—Å—Ç–∞—Ä—ñ–ª—ñ")
                            for item in stale_data_types:
                                logger.info(f"  üîÑ {item}")
                    else:
                        logger.warning("[StageManager] ‚ö†Ô∏è –ö–µ—à –±–µ–∑ —á–∞—Å—É, –æ–Ω–æ–≤–ª—é—î–º–æ...")
                else:
                    logger.warning("[StageManager] ‚ö†Ô∏è –ö–µ—à –ø–æ—Ä–æ–∂–Ω—ñ–π, –æ–Ω–æ–≤–ª—é—î–º–æ...")
                    
            except Exception as e:
                logger.error(f"[StageManager] ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É: {e}")
                logger.info("[StageManager] üîÑ –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ –Ω–æ–≤–∏–º –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è–º...")
        
        logger.info("[StageManager] Starting Stage 1: Data Collection")
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞—î–º–æ –∞–∫—Ç—É–∞–ª—å–Ω–∏–π —á–∞—Å –¥–ª—è last_update_time
        from datetime import datetime
        last_update_time = datetime.now()
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: —Å—Ç–≤–æ—Ä—é—î–º–æ collector –æ–¥–∏–Ω —Ä–∞–∑ —ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ
        from core.stages.stage_1_collectors_layer import IdealStage1Collector
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ collector (—è–∫—â–æ —â–µ –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ)
        if not hasattr(self, '_stage1_collector'):
            self._stage1_collector = IdealStage1Collector(
                tickers=TICKERS,  # –¶–µ —Å–ª–æ–≤–Ω–∏–∫, –Ω–µ —Å–ø–∏—Å–æ–∫!
                timeframes=TIME_FRAMES,
                use_free_data=True,
                enable_cache=True,
                cache_ttl_hours=24,
                last_update_time=last_update_time
            )
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ñ—Å–Ω—É—é—á–∏–π collector
        stage1_data = self._stage1_collector.run_stage_1(
            tickers=TICKERS,  # –¶–µ —Å–ª–æ–≤–Ω–∏–∫, –Ω–µ —Å–ø–∏—Å–æ–∫!
            timeframes=TIME_FRAMES,
            use_free_data=True,
            enable_cache=True,
            cache_ttl_hours=24,
            last_update_time=last_update_time
        )
        
        # –î–æ–¥–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ –≤ –∫–µ—à
        stage1_data['_metadata'] = {
            'last_update_time': last_update_time,
            'collection_time': datetime.now().isoformat()
        }
        
        self.save_cache(cache_path, stage1_data)
        return stage1_data
    
    def run_stage_2(self, stage1_data: Dict[str, Any], force_refresh: bool = False, 
                    tickers: Optional[Dict] = None, time_frames: Optional[List] = None) -> Tuple[Any, Any, Any]:
        """–ïand–ø 2: Data Enrichment"""
        logger.info("[StageManager] DEBUG: –ü–æ—á–∞—Ç–æ–∫ run_stage_2")
        
        # Use provided parameters or fall back to global constants
        if tickers is None:
            from config.config import TICKERS
            tickers = TICKERS
        if time_frames is None:
            from config.config import TIME_FRAMES
            time_frames = TIME_FRAMES
            
        logger.info(f"[StageManager] DEBUG: Using tickers: {list(tickers.keys())}")
        logger.info(f"[StageManager] DEBUG: Using time_frames: {time_frames}")
        
        params = {"stage1_keys": list(stage1_data.keys())}
        params_hash = self.get_params_hash(params)
        cache_path = self.get_cache_path("stage2_enriched", params_hash)
        
        logger.info(f"[StageManager] DEBUG: cache_path: {cache_path}")
        logger.info(f"[StageManager] DEBUG: force_refresh: {force_refresh}")
        
        if not force_refresh and self.is_cache_valid(cache_path):
            logger.info("[StageManager] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –∫–µ—à for Stage 2")
            cached_data = self.load_cache(cache_path)
            logger.info(f"[StageManager] DEBUG: –ó–∞–≤–∞–Ωand–∂–µ–Ω–æ with –∫–µ—à—É: {type(cached_data)}")
            return cached_data
        
        logger.info("[StageManager] –ó–∞–ø—É—Å–∫–∞—é Stage 2: Data Enrichment")
        import os
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–±—Å–æ–ª—é—Ç–Ω–∏–π —à–ª—è—Ö –¥–æ –∫–æ–Ω—Ñand–≥—É—Ä–∞—Üand–π–Ω–æ–≥–æ file—É
        config_path = "config/news_sources.yaml"
        if not os.path.isabs(config_path):
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            config_path = os.path.join(project_root, config_path)
        
        config = load_yaml_config(config_path)
        keyword_dict = config.get("keywords", {})
        
        logger.info(f"[StageManager] DEBUG: keyword_dict keys: {list(keyword_dict.keys())}")
        
        logger.info("[StageManager] DEBUG: Starting run_stage_2_enrichment_fixed...")
        # [FIXED] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ–≤—É —Ñ—É–Ω–∫—Ü—ñ—é —è–∫–∞ –æ–±'—î–¥–Ω—É—î –≤—Å—ñ –¥–∞–Ω—ñ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ —Ç–∞—Ä–≥–µ—Ç–∞–º–∏
        merged_df, metadata = run_stage_2_enrichment_fixed(
            stage1_data=stage1_data,
            tickers=list(tickers.keys()) if isinstance(tickers, dict) else tickers,
            time_frames=time_frames
        )
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –≤ –æ—á—ñ–∫—É–≤–∞–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
        raw_news = stage1_data.get("news", pd.DataFrame())
        pivots = metadata
        
        logger.info(f"[StageManager] DEBUG] run_stage_2_enrichment_fixed completed:")
        logger.info(f"[StageManager] DEBUG] merged_df shape: {merged_df.shape if hasattr(merged_df, 'shape') else 'N/A'}")
        logger.info(f"[StageManager] DEBUG] Target columns: {[col for col in merged_df.columns if 'target' in col.lower()] if hasattr(merged_df, 'columns') else 'N/A'}")
        
        logger.info(f"[StageManager] DEBUG: run_stage_2_enrich_optimized –ø–æ–≤–µ—Ä–Ω—É–≤:")
        logger.info(f"[StageManager] DEBUG: raw_news —Ç–∏–ø: {type(raw_news)}, —Ä–æwith–ºand—Ä: {len(raw_news) if hasattr(raw_news, '__len__') else 'N/A'}")
        logger.info(f"[StageManager] DEBUG: merged_df —Ç–∏–ø: {type(merged_df)}")
        if merged_df is not None:
            logger.info(f"[StageManager] DEBUG: merged_df —Ä–æwith–ºand—Ä: {merged_df.shape}")
            # Check for price columns in merged_df
            close_cols = [col for col in merged_df.columns if 'close' in col.lower()]
            logger.info(f"[StageManager] DEBUG: merged_df close columns: {len(close_cols)}")
            if close_cols:
                logger.info(f"[StageManager] DEBUG: Sample close cols: {close_cols[:3]}")
            else:
                logger.warning(f"[StageManager] DEBUG: No close columns found!")
                logger.info(f"[StageManager] DEBUG: All columns (first 20): {list(merged_df.columns)[:20]}")
        else:
            logger.warning("[StageManager] DEBUG: merged_df is None!")
        logger.info(f"[StageManager] DEBUG: pivots —Ç–∏–ø: {type(pivots)}, –∫–ª—é—áand: {list(pivots.keys()) if isinstance(pivots, dict) else 'N/A'}")
        
        # –ó–±–µ—Äand–≥–∞—î–º–æ –≤ –∫–µ—à
        logger.info("[StageManager] DEBUG: –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à...")
        self.save_cache(cache_path, (raw_news, merged_df, pivots))
        logger.info("[StageManager] DEBUG: –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –∫–µ—à")
        
        # –ó–±–µ—Äand–≥–∞—î–º–æ Stage 2 –¥–∞–Ωand –≤ stages –ø–∞–ø–∫—É
        logger.info("[StageManager] DEBUG: –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è Stage 2 –≤ stages...")
        try:
            from pathlib import Path
            from config.config import PATHS
            
            stages_dir = Path(PATHS["data"]) / "stages"
            stages_dir.mkdir(parents=True, exist_ok=True)
            
            # –ó–±–µ—Äand–≥–∞—î–º–æ merged_df
            if merged_df is not None and not merged_df.empty:
                stage2_path = stages_dir / "stage2_merged.parquet"
                
                # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ problems–Ωand –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ with–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º
                for col in merged_df.columns:
                    if merged_df[col].dtype == 'object':
                        try:
                            merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')
                        except:
                            merged_df[col] = merged_df[col].astype(str)
                
                merged_df.to_parquet(stage2_path, index=False)
                logger.info(f"[StageManager] Saved stage2_merged.parquet: {merged_df.shape}")
            
            # –ó–±–µ—Äand–≥–∞—î–º–æ pivots —è–∫ JSON
            if pivots:
                import json
                pivots_path = stages_dir / "stage2_pivots.json"
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ DataFrame –≤ dict for JSON
                serializable_pivots = {}
                for interval, pivot in pivots.items():
                    if hasattr(pivot, 'to_dict'):
                        serializable_pivots[interval] = pivot.to_dict()
                    else:
                        serializable_pivots[interval] = pivot
                
                with open(pivots_path, 'w') as f:
                    json.dump(serializable_pivots, f, default=str)
                logger.info(f"[StageManager] Saved stage2_pivots.json: {list(pivots.keys())}")
                
        except Exception as e:
            logger.error(f"[StageManager] Error saving Stage 2: {e}")
        
        # –ù–∞–∫–æ–ø–∏—á—É—î–º–æ –¥–∞–Ωand for Colab (–ø–µ—Ä–µ–¥ –≤–∏–¥–∞–ª–µ–Ω–Ω—è–º –∫–æ–ª–æ–Ω–æ–∫)
        logger.info("[StageManager] DEBUG: –í–∏–∫–ª–∏–∫ _accumulate_stage2_data...")
        self._accumulate_stage2_data(raw_news, merged_df, pivots)
        logger.info("[StageManager] DEBUG: _accumulate_stage2_data completed")
        
        return raw_news, merged_df, pivots
    
    def _accumulate_stage2_data(self, raw_news, merged_df, pivots):
        """–ù–∞–∫–æ–ø–∏—á—É—î –¥–∞–Ωand –µand–ø—É 2 for –µ–∫—Å–ø–æ—Ä—Ç—É –≤ Colab"""
        logger.info("[StageManager] DEBUG: –ü–æ—á–∞—Ç–æ–∫ —Ñ—É–Ω–∫—Üand—ó –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è")
        
        try:
            import os
            import pandas as pd
            from pathlib import Path
            
            # DEBUG: –ü–µ—Ä–µ–≤and—Ä—è—î–º–æ –≤—Öand–¥–Ωand –¥–∞–Ωand
            logger.info(f"[StageManager] DEBUG: raw_news —Ç–∏–ø: {type(raw_news)}, —Ä–æwith–ºand—Ä: {len(raw_news) if hasattr(raw_news, '__len__') else 'N/A'}")
            logger.info(f"[StageManager] DEBUG: merged_df —Ç–∏–ø: {type(merged_df)}")
            if merged_df is not None:
                logger.info(f"[StageManager] DEBUG: merged_df —Ä–æwith–ºand—Ä: {merged_df.shape}")
                logger.info(f"[StageManager] DEBUG: merged_df –∫–æ–ª–æ–Ω–∫–∏: {list(merged_df.columns)[:10]}")
            else:
                logger.warning("[StageManager] DEBUG: merged_df is None")
                return
            
            # –ü–µ—Ä–µ–≤and—Ä—è—î–º–æ —á–∏ —î –¥–∞–Ωand for –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è
            if merged_df is None or merged_df.empty:
                logger.warning("[StageManager] –ù–µ–º–∞—î data for –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è")
                return
            
            # –í–ê–õ–Ü–î–ê–¶–Ü–Ø: –ü–µ—Ä–µ–≤and—Ä—è—î–º–æ –Ω–∞—è–≤–Ωand—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö —Ñand—á
            critical_features = ['RSI', 'SMA', 'gap', 'target']
            missing_features = []
            
            for feature in critical_features:
                feature_cols = [col for col in merged_df.columns if feature.lower() in col.lower()]
                if len(feature_cols) == 0:
                    missing_features.append(feature)
                else:
                    logger.info(f"[StageManager] [OK] {feature}: {len(feature_cols)} columns found")
            
            if missing_features:
                logger.warning(f"[StageManager] [ERROR] Missing critical features: {missing_features}")
                logger.warning(f"[StageManager] [WARN] Accumulated dataset will be incomplete for ML!")
            else:
                logger.info("[StageManager] [OK] All critical features present for ML")
            
            # –®–ª—è—Ö for –Ω–∞–∫–æ–ø–∏—á–µ–Ω–∏—Ö data
            accumulated_dir = Path("data/colab/accumulated")
            logger.info(f"[StageManager] DEBUG: –°—Ç–≤–æ—Ä—é—é –ø–∞–ø–∫—É: {accumulated_dir}")
            accumulated_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"[StageManager] DEBUG: –ü–∞–ø–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–∞ —É—Å–øand—à–Ω–æ")
            
            # –§–∞–π–ª for –Ω–∞–∫–æ–ø–∏—á–µ–Ω–∏—Ö data
            accumulated_file = accumulated_dir / "stage2_accumulated.parquet"
            logger.info(f"[StageManager] DEBUG: –§–∞–π–ª for with–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {accumulated_file}")
            
            # –ü–µ—Ä–µ–≤and—Ä—è—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ for removing duplicates
            duplicate_cols = []
            if 'published_at' in merged_df.columns:
                duplicate_cols.append('published_at')
            if 'title' in merged_df.columns:
                duplicate_cols.append('title')
            elif 'url' in merged_df.columns:
                duplicate_cols.append('url')
            
            logger.info(f"[StageManager] DEBUG: –ö–æ–ª–æ–Ω–∫–∏ for –¥—É–±–ªand–∫–∞—Çand–≤: {duplicate_cols}")
            
            # –ó–∞–≤–∞–Ωand–∂—É—î–º–æ and—Å–Ω—É—é—áand –¥–∞–Ωand
            if accumulated_file.exists():
                logger.info("[StageManager] DEBUG: –§–∞–π–ª and—Å–Ω—É—î, for–≤–∞–Ωand–∂—É—é...")
                existing_df = pd.read_parquet(accumulated_file)
                logger.info(f"[StageManager] DEBUG: –Ü—Å–Ω—É—é—áand –¥–∞–Ωand: {existing_df.shape}")
                # –û–±'—î–¥–Ω—É—î–º–æ with –Ω–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏
                combined_df = pd.concat([existing_df, merged_df], ignore_index=True)
                logger.info(f"[StageManager] DEBUG: –û–±'—î–¥–Ω–∞–Ωand –¥–∞–Ωand: {combined_df.shape}")
                # –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ªand–∫–∞—Ç–∏ —è–∫—â–æ —î –≤and–¥–ø–æ–≤and–¥–Ωand –∫–æ–ª–æ–Ω–∫–∏
                if duplicate_cols:
                    before_dedup = len(combined_df)
                    combined_df = combined_df.drop_duplicates(subset=duplicate_cols, keep='last')
                    after_dedup = len(combined_df)
                    logger.info(f"[StageManager] DEBUG: –í–∏–¥–∞–ª–µ–Ω–æ –¥—É–±–ªand–∫–∞—Çand–≤: {before_dedup - after_dedup}")
                logger.info(f"[StageManager] –û–±'—î–¥–Ω–∞–Ω–æ with and—Å–Ω—É—é—á–∏–º–∏ –¥–∞–Ω–∏–º–∏")
            else:
                combined_df = merged_df
                logger.info(f"[StageManager] –°—Ç–≤–æ—Ä–µ–Ω–æ –Ω–æ–≤–∏–π file –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è, —Ä–æwith–ºand—Ä: {combined_df.shape}")
            
            # –ó–±–µ—Äand–≥–∞—î–º–æ –Ω–∞–∫–æ–ø–∏—á–µ–Ωand –¥–∞–Ωand
            logger.info("[StageManager] DEBUG: –ó–±–µ—Äand–≥–∞—é file...")
            
            # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ problems–Ωand –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ with–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º
            for col in combined_df.columns:
                if combined_df[col].dtype == 'object':
                    try:
                        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')
                    except:
                        combined_df[col] = combined_df[col].astype(str)
            
            combined_df.to_parquet(accumulated_file)
            logger.info(f"[StageManager] DEBUG: File saved successfully")
            logger.info(f"[StageManager] Accumulated {len(combined_df)} rows of stage 2 data")
            
        except Exception as e:
            logger.error(f"[StageManager] Data accumulation error: {e}")
            import traceback
            logger.error(f"[StageManager] Traceback: {traceback.format_exc()}")
    
    def run_stage_3(self, stage2_data: Tuple[Any, Any, Any], force_refresh: bool = False) -> Tuple[Any, Any, Any, Any]:
        """–ï—Ç–∞–ø 3: Feature Engineering"""
        # –í–ò–ü–†–ê–í–õ–ï–ù–û - –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ç–∏–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: Tuple[Any, Any, Any]
        raw_news, enhanced_data, metadata = stage2_data
        
        params = {"enhanced_shape": enhanced_data.shape, "columns": list(enhanced_data.columns)[:10]}
        params_hash = self.get_params_hash(params)
        cache_path = self.get_cache_path("stage3_features", params_hash)
        
        if not force_refresh and self.is_cache_valid(cache_path):
            logger.info("[StageManager] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –∫–µ—à for Stage 3")
            return self.load_cache(cache_path)
        
        logger.info("[StageManager] –ó–∞–ø—É—Å–∫–∞—é Stage 3: Feature Engineering")
        calendar = TradingCalendar.from_year(2025)
        
        stage1_data = {}  # –ú–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –¥–∞–Ω—ñ –∑ –µ—Ç–∞–ø—É 1 —è–∫—â–æ needed
        stage2_data_formatted = {'merged_data': enhanced_data}  # –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç
        config = {'calendar': calendar}
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–æ–∑–ø–∞–∫–æ–≤—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑ Dict
        stage3_result = prepare_stage3_datasets(stage1_data, stage2_data_formatted, config)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        if isinstance(stage3_result, dict):
            merged_stage3 = stage3_result.get('features', {}).get('technical', pd.DataFrame())
            context_df = stage3_result.get('context', pd.DataFrame())
            features_df = stage3_result.get('features', {}).get('technical', pd.DataFrame())
            trigger_data = stage3_result.get('triggers', pd.DataFrame())
        else:
            # –Ø–∫—â–æ –ø–æ–≤–µ—Ä–Ω—É—Ç–æ –∫–æ—Ä—Ç–µ–∂ (—Å—Ç–∞—Ä–∏–π —Ñ–æ—Ä–º–∞—Ç)
            merged_stage3, context_df, features_df, trigger_data = stage3_result
        
        self.save_cache(cache_path, (merged_stage3, context_df, features_df, trigger_data))
        return merged_stage3, context_df, features_df, trigger_data
    
    def run_stage_4(self, features_df: pd.DataFrame, models: Optional[list] = None, force_refresh: bool = False) -> pd.DataFrame:
        params = {"features_shape": features_df.shape, "models": models}
        params_hash = self.get_params_hash(params)
        cache_path = self.get_cache_path("stage4_models", params_hash)
        
        if not force_refresh and self.is_cache_valid(cache_path):
            logger.info("[StageManager] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –∫–µ—à for Stage 4")
            return self.load_cache(cache_path)
        
        logger.info("[StageManager] –ó–∞–ø—É—Å–∫–∞—é Stage 4: Model Training")
        results_df = benchmark_all_models(features_df, models=models)
        
        self.save_cache(cache_path, results_df)
        return results_df
    
    def run_pipeline_incremental(self, 
                                stage_to_run: str = None,
                                debug_no_network: bool = False,
                                models: Optional[list] = None,
                                force_refresh: bool = False) -> Dict[str, Any]:
        """"
        –ó–∞–ø—É—Å–∫–∞—î pipeline and–Ω–∫—Ä–µ–º–µ–Ωand–ª—å–Ω–æ
        
        Args:
            stage_to_run: '1', '2', '3', '4', 'all' - which –µand–ø for–ø—É—Å–∫–∞—Ç–∏
            debug_no_network: —á–∏ use —Ä–µ–∞–ª—å–Ωand for–ø–∏—Ç–∏
            models: —Å–ø–∏—Å–æ–∫ –º–æwhere–ª–µ–π for –µand–ø—É 4
            force_refresh: —á–∏ –ø—Ä–∏–º—É—Å–æ–≤–æ –æ–Ω–æ–≤–∏—Ç–∏ –∫–µ—à
            use_cache: —á–∏ use –∫–µ—à
            
        Returns:
            Dict with —Ä–µwith—É–ª—åandand–º–∏ all—Ö –µand–øand–≤
        """
        logger.info(f"[StageManager] Pipeline: –ü–æ—á–∞—Ç–æ–∫, stage_to_run={stage_to_run}")
        results = {}
        
        # –ïand–ø 1
        if stage_to_run in ['1', 'all', None]:
            results['stage1'] = self.run_stage_1(debug_no_network, force_refresh)
        
        # –ïand–ø 2
        if stage_to_run in ['2', 'all', None]:
            # –Ø–∫—â–æ for–ø—É—Å–∫–∞—î–º–æ —Çand–ª—å–∫–∏ –µand–ø 2, –ø–æ—Ç—Äand–±–µ–Ω –µand–ø 1
            if stage_to_run == '2' and 'stage1' not in results:
                logger.info("[StageManager] Pipeline: –ïand–ø 2 –ø–æ—Ç—Ä–µ–±—É—î –µand–ø—É 1, for–ø—É—Å–∫–∞—é...")
                results['stage1'] = self.run_stage_1(debug_no_network, force_refresh)
            
            if 'stage1' in results:
                logger.info(f"[StageManager] Pipeline: –ó–∞–ø—É—Å–∫–∞—é –µand–ø 2 with force_refresh={force_refresh}")
                raw_news, merged_df, pivots = self.run_stage_2(results['stage1'], force_refresh)
            results['stage2'] = {
                'raw_news': raw_news,
                'merged_df': merged_df,
                'pivots': pivots
            }
        
        # –ï—Ç–∞–ø 3
        if stage_to_run in ['3', 'all', None] and 'stage2' in results:
            stage2_data_tuple = (results['stage2']['raw_news'], results['stage2']['merged_df'], results['stage2']['pivots'])
            merged_stage3, context_df, features_df, trigger_data = self.run_stage_3(stage2_data_tuple, force_refresh)
            results['stage3'] = {
                'merged_stage3': merged_stage3,
                'context_df': context_df,
                'features_df': features_df,
                'trigger_data': trigger_data
            }
        
        # –ïand–ø 4
        if stage_to_run in ['4', 'all', None] and 'stage3' in results:
            results_df = self.run_stage_4(results['stage3']['features_df'], models, force_refresh)
            results['stage4'] = results_df
        
        logger.info(f"[StageManager] Pipeline for–≤–µ—Ä—à–µ–Ω–æ. –ïand–ø–∏: {list(results.keys())}")
        return results
    
    def export_for_colab(self, stage: str = '2', results: Dict[str, Any] = None) -> str:
        """
        –ï–∫—Å–ø–æ—Ä—Ç—É—î –¥–∞–Ωand for Colab
        
        Args:
            stage: '2' or '3' - which –µand–ø –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏
            results: —Ä–µwith—É–ª—åand—Ç–∏ pipeline
            
        Returns:
            –®–ª—è—Ö –¥–æ –µ–∫—Å–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ–≥–æ file—É
        """
        if stage == '2' and results and 'stage2' in results:
            merged_df = results['stage2']['merged_df']
            return colab_utils.export_stage2_data(merged_df)
        elif stage == '3' and results and 'stage3' in results:
            stage3_data = results['stage3']
            return colab_utils.export_stage3_data(
                stage3_data['features_df'],
                stage3_data['context_df'],
                stage3_data['trigger_data']
            )
        elif stage == '4' and results and 'stage4' in results:
            results_df = results['stage4']
            return colab_utils.export_stage4_data(results_df)
        else:
            raise ValueError(f"–ù–µ–º–æ–∂–ª–∏–≤–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –µand–ø {stage}")
    
    def import_from_colab(self, results_file: str) -> pd.DataFrame:
        """
        –Ü–º–ø–æ—Ä—Ç—É—î —Ä–µwith—É–ª—åand—Ç–∏ with Colab
        
        Args:
            results_file: –®–ª—è—Ö –¥–æ file—É with —Ä–µwith—É–ª—åandand–º–∏
            
        Returns:
            DataFrame with —Ä–µwith—É–ª—åandand–º–∏ –º–æwhere–ª–µ–π
        """
        try:
            return colab_utils.import_colab_results(results_file)
        except Exception as e:
            logger.error(f"[StageManager] Error and–º–ø–æ—Ä—Ç—É —Ä–µwith—É–ª—åand—Çand–≤ with Colab: {e}")
            import traceback
            logger.error(f"[StageManager] Traceback: {traceback.format_exc()}")
            return None
    
    def create_colab_template(self, output_path: str = "colab_template.ipynb") -> str:
        """
        –°—Ç–≤–æ—Ä—é—î —à–∞–±–ª–æ–Ω Colab notebook
        
        Args:
            output_path: –®–ª—è—Ö for with–±–µ—Ä–µ–∂–µ–Ω–Ω—è —à–∞–±–ª–æ–Ω—É
            
        Returns:
            –®–ª—è—Ö –¥–æ created–≥–æ —à–∞–±–ª–æ–Ω—É
        """
        try:
            return colab_utils.create_colab_notebook_template(output_path)
        except Exception as e:
            logger.error(f"[StageManager] Error —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —à–∞–±–ª–æ–Ω—É Colab: {e}")
            import traceback
            logger.error(f"[StageManager] Traceback: {traceback.format_exc()}")
            return None
    
    def clear_cache(self, stage: str = None):
        """–û—á–∏—â—É—î –∫–µ—à"""
        try:
            if stage:
                cache_dir = os.path.join(self.base_path, stage)
                if os.path.exists(cache_dir):
                    for file in os.listdir(cache_dir):
                        os.remove(os.path.join(cache_dir, file))
                    logger.info(f"[StageManager] –û—á–∏—â–µ–Ω–æ –∫–µ—à for {stage}")
            else:
                for dir_name in os.listdir(self.base_path):
                    dir_path = os.path.join(self.base_path, dir_name)
                    if os.path.isdir(dir_path):
                        for file in os.listdir(dir_path):
                            os.remove(os.path.join(dir_path, file))
                logger.info("[StageManager] –û—á–∏—â–µ–Ω–æ –≤–µ—Å—å –∫–µ—à")
        except Exception as e:
            logger.error(f"[StageManager] Error –æ—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É: {e}")
            import traceback
            logger.error(f"[StageManager] Traceback: {traceback.format_exc()}")
            cache_dir = os.path.join(self.base_path, stage)
            if os.path.exists(cache_dir):
                for file in os.listdir(cache_dir):
                    os.remove(os.path.join(cache_dir, file))
                logger.info(f"[StageManager] –û—á–∏—â–µ–Ω–æ –∫–µ—à for {stage}")
        else:
            for dir_name in os.listdir(self.base_path):
                dir_path = os.path.join(self.base_path, dir_name)
                if os.path.isdir(dir_path):
                    for file in os.listdir(dir_path):
                        os.remove(os.path.join(dir_path, file))
            logger.info("[StageManager] –û—á–∏—â–µ–Ω–æ –≤–µ—Å—å –∫–µ—à")

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫with–µ–º–ø–ª—è—Ä
stage_manager = StageManager()