#!/usr/bin/env python3
"""
Final Unified Trading Pipeline - Complete Implementation
–†–µ–∞–ª—ñ–∑—É—î –ø–æ–≤–Ω—É –ª–æ–≥—ñ–∫—É –æ–ø–∏—Å–∞–Ω—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º –∑ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–æ—é –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é
"""

import os
import json
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Tuple, List, Union
from pathlib import Path
import pandas as pd
import numpy as np
import logging

from utils.logger_fixed import ProjectLogger
from config.config import TICKERS, TIME_FRAMES

logger = ProjectLogger.get_logger("FinalPipeline")

class DataParser:
    """–ï—Ç–∞–ø 1: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö"""
    
    def __init__(self, cache_dir: str = "data/cache/raw"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def parse_all_data(self, force_refresh: bool = False) -> Dict[str, Any]:
        """–ó–±–∏—Ä–∞—î –≤—Å—ñ —Å–∏—Ä–∏ –¥–∞–Ω—ñ (–Ω–æ–≤–∏–Ω–∏, —Ü—ñ–Ω–∏, –º–∞–∫—Ä–æ)"""
        logger.info("[DataParser] –ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö...")
        
        cache_file = self.cache_dir / "raw_data.pkl"
        
        if not force_refresh and cache_file.exists():
            try:
                cached_data = pickle.load(open(cache_file, 'rb'))
                if self._is_data_fresh(cached_data):
                    logger.info("[DataParser] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–µ—à–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ")
                    return cached_data
            except Exception as e:
                logger.warning(f"[DataParser] –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É: {e}")
        
        # –ó–±–∏—Ä–∞—î–º–æ —Å–≤—ñ–∂—ñ –¥–∞–Ω—ñ
        from core.stages.stage_1_collectors_layer import IdealStage1Collector
        
        collector = IdealStage1Collector(
            tickers=TICKERS,
            timeframes=TIME_FRAMES,
            use_free_data=True,
            enable_cache=True
        )
        
        raw_data = collector.run_stage_1(
            tickers=TICKERS,
            timeframes=TIME_FRAMES,
            use_free_data=True,
            enable_cache=True
        )
        
        # –î–æ–¥–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
        raw_data['_metadata'] = {
            'parsing_time': datetime.now(),
            'tickers': list(TICKERS.keys()),
            'timeframes': TIME_FRAMES
        }
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–µ—à
        pickle.dump(raw_data, open(cache_file, 'wb'))
        logger.info(f"[DataParser] –ó—ñ–±—Ä–∞–Ω–æ –¥–∞–Ω–∏—Ö: {list(raw_data.keys())}")
        
        return raw_data
    
    def _is_data_fresh(self, data: Dict[str, Any], max_age_hours: int = 1) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –¥–∞–Ω—ñ —Å–≤—ñ–∂—ñ"""
        if '_metadata' not in data:
            return False
        
        parsing_time = data['_metadata'].get('parsing_time')
        if not parsing_time:
            return False
        
        age = datetime.now() - parsing_time
        return age.total_seconds() < max_age_hours * 3600

class DataEnricher:
    """–ï—Ç–∞–ø 2: –ó–±–∞–≥–∞—á–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –º–∞–∫—Ä–æ, –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏, —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–æ–º"""
    
    def __init__(self, cache_dir: str = "data/cache/enriched"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def enrich_data(self, raw_data: Dict[str, Any], force_refresh: bool = False) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """–ó–±–∞–≥–∞—á—É—î —Å–∏—Ä–∏ –¥–∞–Ω—ñ —Ç–µ—Ö–Ω—ñ—á–Ω–∏–º–∏ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏, –º–∞–∫—Ä–æ, —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–æ–º"""
        logger.info("[DataEnricher] –ü–æ—á–∞—Ç–æ–∫ –∑–±–∞–≥–∞—á–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")
        
        cache_file = self.cache_dir / "enriched_data.pkl"
        
        if not force_refresh and cache_file.exists():
            try:
                cached_data = pickle.load(open(cache_file, 'rb'))
                if self._is_enrichment_fresh(cached_data):
                    logger.info("[DataEnricher] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–µ—à–æ–≤–∞–Ω—ñ –∑–±–∞–≥–∞—á–µ–Ω—ñ –¥–∞–Ω—ñ")
                    return cached_data['merged_df'], cached_data['metadata']
            except Exception as e:
                logger.warning(f"[DataEnricher] –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É: {e}")
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –∑–±–∞–≥–∞—á–µ–Ω–Ω—è
        from core.stages.stage_2_enrichment import run_stage_2_enrich_optimized
        
        raw_news, merged_df, pivots = run_stage_2_enrich_optimized(
            stage1_data=raw_data,
            tickers=TICKERS,
            time_frames=TIME_FRAMES
        )
        
        # –î–æ–¥–∞—î–º–æ –∫–æ–≤–∑–Ω—ñ —Å–µ—Ä–µ–¥–Ω—ñ —Ç–∞ —ñ–Ω—à—ñ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫–∏
        merged_df = self._add_calculations(merged_df)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
        metadata = {
            'enrichment_time': datetime.now(),
            'raw_news_count': len(raw_news) if raw_news else 0,
            'merged_shape': merged_df.shape if merged_df is not None else None,
            'pivots': list(pivots.keys()) if pivots else [],
            'features_added': self._detect_added_features(raw_data, merged_df),
            'calculations_added': ['moving_averages', 'price_changes', 'volatility']
        }
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–µ—à
        enriched_data = {
            'merged_df': merged_df,
            'metadata': metadata,
            'raw_news': raw_news,
            'pivots': pivots
        }
        pickle.dump(enriched_data, open(cache_file, 'wb'))
        
        logger.info(f"[DataEnricher] –ó–±–∞–≥–∞—á–µ–Ω–æ –¥–∞–Ω–∏—Ö: {merged_df.shape if merged_df is not None else None}")
        return merged_df, metadata
    
    def _add_calculations(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–¥–∞—î –∫–æ–≤–∑–Ω—ñ —Å–µ—Ä–µ–¥–Ω—ñ —Ç–∞ —ñ–Ω—à—ñ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫–∏ –¥–æ DataFrame"""
        if df is None or df.empty:
            return df
        
        logger.info("[DataEnricher] –î–æ–¥–∞–≤–∞–Ω–Ω—è –∫–æ–≤–∑–Ω–∏—Ö —Å–µ—Ä–µ–¥–Ω—ñ—Ö —Ç–∞ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—ñ–≤...")
        
        # –ö–æ–≤–∑–Ω—ñ —Å–µ—Ä–µ–¥–Ω—ñ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –ø–µ—Ä—ñ–æ–¥—ñ–≤
        for period in [5, 10, 20, 50]:
            df[f'sma_{period}'] = df['close'].rolling(window=period).mean()
            df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi_14'] = 100 - (100 / (1 + rs))
        
        # MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å
        df['volatility_20'] = df['close'].rolling(window=20).std()
        df['volatility_ratio'] = df['volatility_20'] / df['volatility_20'].rolling(window=50).mean()
        
        # –ó–º—ñ–Ω–∏ —Ü—ñ–Ω
        df['price_change_1d'] = df['close'].pct_change(1)
        df['price_change_5d'] = df['close'].pct_change(5)
        df['price_change_abs'] = abs(df['price_change_1d'])
        
        logger.info(f"[DataEnricher] –î–æ–¥–∞–Ω–æ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—ñ–≤: {df.shape}")
        return df
    
    def _is_enrichment_fresh(self, data: Dict[str, Any], max_age_hours: int = 6) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –∑–±–∞–≥–∞—á–µ–Ω—ñ –¥–∞–Ω—ñ —Å–≤—ñ–∂—ñ"""
        enrichment_time = data.get('metadata', {}).get('enrichment_time')
        if not enrichment_time:
            return False
        
        age = datetime.now() - enrichment_time
        return age.total_seconds() < max_age_hours * 3600
    
    def _detect_added_features(self, raw_data: Dict[str, Any], enriched_df: pd.DataFrame) -> List[str]:
        """–í–∏–∑–Ω–∞—á–∞—î —è–∫—ñ —Ñ—ñ—á—ñ –±—É–ª–∏ –¥–æ–¥–∞–Ω—ñ –ø—ñ–¥ —á–∞—Å –∑–±–∞–≥–∞—á–µ–Ω–Ω—è"""
        if enriched_df is None:
            return []
        
        feature_categories = {
            'technical': ['rsi', 'sma', 'ema', 'macd', 'bollinger'],
            'macro': ['gdp', 'inflation', 'unemployment', 'interest'],
            'sentiment': ['sentiment', 'news_score', 'keywords'],
            'volume': ['volume', 'volatility'],
            'price_action': ['gap', 'pivot', 'support', 'resistance']
        }
        
        added_features = []
        columns_lower = [col.lower() for col in enriched_df.columns]
        
        for category, keywords in feature_categories.items():
            if any(keyword in ' '.join(columns_lower) for keyword in keywords):
                added_features.append(category)
        
        return added_features

class FeatureSelector:
    """–ï—Ç–∞–ø 3: –ì–Ω—É—á–∫–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–∏–±–æ—Ä—É —Ñ—ñ—á –ø—ñ–¥ –∫–æ–∂–µ–Ω —Ç—ñ–∫–µ—Ä, —Ç–∞–π–º—Ñ—Ä–µ–π–º, —Ç–∞—Ä–≥–µ—Ç"""
    
    def __init__(self, cache_dir: str = "data/cache/features"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def prepare_features(self, enriched_df: pd.DataFrame, 
                        target_config: Optional[Dict] = None,
                        force_refresh: bool = False) -> Dict[str, Any]:
        """–ì–æ—Ç—É—î –≥–Ω—É—á–∫—ñ —Ñ—ñ—á—ñ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∞—Ä–≥–µ—Ç—ñ–≤"""
        logger.info("[FeatureSelector] –ü–æ—á–∞—Ç–æ–∫ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ñ—ñ—á...")
        
        if target_config is None:
            target_config = self._get_default_target_config()
        
        cache_file = self.cache_dir / f"features_{hashlib.md5(str(target_config).encode()).hexdigest()[:8]}.pkl"
        
        if not force_refresh and cache_file.exists():
            try:
                cached_data = pickle.load(open(cache_file, 'rb'))
                logger.info("[FeatureSelector] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–µ—à–æ–≤–∞–Ω—ñ —Ñ—ñ—á—ñ")
                return cached_data
            except Exception as e:
                logger.warning(f"[FeatureSelector] –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É: {e}")
        
        # –ì–æ—Ç—É—î–º–æ —Ñ—ñ—á—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –µ—Ç–∞–ø 3
        from core.stages.stage_3_features import prepare_stage3_datasets
        
        stage1_data = {}
        stage2_data = {'merged_data': enriched_df}
        config = {'targets': target_config}
        
        feature_results = prepare_stage3_datasets(stage1_data, stage2_data, config)
        
        # –û—Ä–≥–∞–Ω—ñ–∑–æ–≤—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        features_dict = {
            'features_by_target': {},
            'context_data': {},
            'metadata': {
                'preparation_time': datetime.now(),
                'target_config': target_config,
                'feature_count': len(enriched_df.columns) if enriched_df is not None else 0
            }
        }
        
        if isinstance(feature_results, dict):
            features_dict['features_by_target'] = feature_results.get('features', {})
            features_dict['context_data'] = feature_results.get('context', pd.DataFrame())
        else:
            merged_stage3, context_df, features_df, trigger_data = feature_results
            features_dict['features_by_target'] = {'default': features_df}
            features_dict['context_data'] = context_df
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–µ—à
        pickle.dump(features_dict, open(cache_file, 'wb'))
        
        total_features = sum(len(df.columns) if hasattr(df, 'columns') else 0 
                           for df in features_dict['features_by_target'].values())
        logger.info(f"[FeatureSelector] –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {total_features} —Ñ—ñ—á –¥–ª—è —Ç–∞—Ä–≥–µ—Ç—ñ–≤")
        
        return features_dict
    
    def _get_default_target_config(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º—É—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é —Ç–∞—Ä–≥–µ—Ç—ñ–≤ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º"""
        return {
            'price_direction': {
                'threshold': 0.02,
                'lookahead_periods': [1, 3, 5],
                'noise_filter': 0.005
            },
            'price_change_pct': {
                'threshold': 0.01,
                'lookahead_periods': [1, 3, 5],
                'noise_filter': 0.003
            },
            'volatility_target': {
                'threshold': 0.015,
                'lookahead_periods': [1, 3],
                'noise_filter': 0.002
            }
        }

class ModelTrainer:
    """–ï—Ç–∞–ø 4: –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º –ª–æ–∫–∞–ª—å–Ω–∏—Ö/Colab"""
    
    def __init__(self, cache_dir: str = "data/cache/models"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.models_dir = Path("data/models")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
    def train_models(self, features_dict: Dict[str, Any], 
                    model_config: Optional[Dict] = None,
                    force_refresh: bool = False) -> Dict[str, Any]:
        """–¢—Ä–µ–Ω—É—î –º–æ–¥–µ–ª—ñ –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º –ª–æ–∫–∞–ª—å–Ω–∏—Ö/Colab"""
        logger.info("[ModelTrainer] –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π...")
        
        if model_config is None:
            model_config = self._get_default_model_config()
        
        cache_file = self.cache_dir / f"training_results_{hashlib.md5(str(model_config).encode()).hexdigest()[:8]}.pkl"
        
        if not force_refresh and cache_file.exists():
            try:
                cached_results = pickle.load(open(cache_file, 'rb'))
                if self._are_models_fresh(cached_results):
                    logger.info("[ModelTrainer] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–µ—à–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ")
                    return cached_results
            except Exception as e:
                logger.warning(f"[ModelTrainer] –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É: {e}")
        
        training_results = {
            'light_models': {},
            'heavy_models': {},
            'metadata': {
                'training_time': datetime.now(),
                'model_config': model_config
            }
        }
        
        # –¢—Ä–µ–Ω—É—î–º–æ –ª–µ–≥–∫—ñ –º–æ–¥–µ–ª—ñ –ª–æ–∫–∞–ª—å–Ω–æ
        for target_name, features_df in features_dict['features_by_target'].items():
            if features_df is not None and not features_df.empty:
                light_results = self._train_light_models(features_df, target_name, model_config)
                training_results['light_models'][target_name] = light_results
                
                # –ì–æ—Ç—É—î–º–æ –≤–∞–∂–∫—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è Colab
                heavy_data = self._prepare_heavy_models_data(features_df, target_name)
                training_results['heavy_models'][target_name] = heavy_data
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–µ—à
        pickle.dump(training_results, open(cache_file, 'wb'))
        
        logger.info(f"[ModelTrainer] –¢—Ä–µ–Ω–æ–≤–∞–Ω–æ {len(training_results['light_models'])} –ª–µ–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π")
        return training_results
    
    def _train_light_models(self, features_df: pd.DataFrame, target_name: str, 
                           model_config: Dict) -> Dict[str, Any]:
        """–¢—Ä–µ–Ω—É—î –ª–µ–≥–∫—ñ –º–æ–¥–µ–ª—ñ –ª–æ–∫–∞–ª—å–Ω–æ"""
        from core.stages.stage_4_benchmark import benchmark_all_models
        
        light_models = [model for model in model_config.get('light_models', []) 
                       if model in ['linear', 'random_forest', 'xgboost', 'lightgbm']]
        
        try:
            results = benchmark_all_models(features_df, models=light_models)
            return {
                'results': results,
                'model_type': 'light',
                'target': target_name,
                'training_time': datetime.now()
            }
        except Exception as e:
            logger.error(f"[ModelTrainer] –ü–æ–º–∏–ª–∫–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ª–µ–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è {target_name}: {e}")
            return {'error': str(e), 'model_type': 'light', 'target': target_name}
    
    def _prepare_heavy_models_data(self, features_df: pd.DataFrame, target_name: str) -> Dict[str, Any]:
        """–ì–æ—Ç—É—î –¥–∞–Ω—ñ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –≤–∞–∂–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –≤ Colab"""
        colab_dir = Path("data/colab/for_training")
        colab_dir.mkdir(parents=True, exist_ok=True)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ—ñ—á—ñ –¥–ª—è Colab
        features_file = colab_dir / f"features_{target_name}.parquet"
        features_df.to_parquet(features_file)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        training_config = {
            'target_name': target_name,
            'features_shape': features_df.shape,
            'features_file': str(features_file),
            'heavy_models': ['lstm', 'transformer', 'bert', 'gpt'],
            'training_params': {
                'epochs': 100,
                'batch_size': 32,
                'learning_rate': 0.001,
                'validation_split': 0.2
            }
        }
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
        config_file = colab_dir / f"config_{target_name}.json"
        with open(config_file, 'w') as f:
            json.dump(training_config, f, indent=2)
        
        return {
            'config': training_config,
            'features_file': str(features_file),
            'config_file': str(config_file),
            'model_type': 'heavy',
            'target': target_name,
            'preparation_time': datetime.now()
        }
    
    def _get_default_model_config(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º—É—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –º–æ–¥–µ–ª–µ–π –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º"""
        return {
            'light_models': ['linear', 'random_forest', 'xgboost', 'lightgbm'],
            'heavy_models': ['lstm', 'transformer', 'bert', 'gpt'],
            'evaluation_metrics': ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc'],
            'cross_validation_folds': 5,
            'test_size': 0.2,
            'random_state': 42
        }
    
    def _are_models_fresh(self, cached_results: Dict[str, Any], max_age_hours: int = 24) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –º–æ–¥–µ–ª—ñ —Å–≤—ñ–∂—ñ"""
        training_time = cached_results.get('metadata', {}).get('training_time')
        if not training_time:
            return False
        
        age = datetime.now() - training_time
        return age.total_seconds() < max_age_hours * 3600

class ModelComparator:
    """–ï—Ç–∞–ø 5: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ñ—ñ–Ω–∞–ª—å–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤"""
    
    def __init__(self, cache_dir: str = "data/cache/comparison"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def compare_models(self, training_results: Dict[str, Any], 
                       force_refresh: bool = False) -> Dict[str, Any]:
        """–ü–æ—Ä—ñ–≤–Ω—é—î –º–æ–¥–µ–ª—ñ —Ç–∞ –≤–∏–±–∏—Ä–∞—î –Ω–∞–π–∫—Ä–∞—â—ñ"""
        logger.info("[ModelComparator] –ü–æ—á–∞—Ç–æ–∫ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π...")
        
        cache_file = self.cache_dir / "comparison_results.pkl"
        
        if not force_refresh and cache_file.exists():
            try:
                cached_results = pickle.load(open(cache_file, 'rb'))
                if self._is_comparison_fresh(cached_results):
                    logger.info("[ModelComparator] –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–µ—à–æ–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è")
                    return cached_results
            except Exception as e:
                logger.warning(f"[ModelComparator] –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É: {e}")
        
        comparison_results = {
            'best_models': {},
            'model_rankings': {},
            'performance_metrics': {},
            'final_signals': {},
            'metadata': {
                'comparison_time': datetime.now(),
                'total_models_compared': 0
            }
        }
        
        # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –ª–µ–≥–∫—ñ –º–æ–¥–µ–ª—ñ
        for target_name, light_results in training_results['light_models'].items():
            if 'error' not in light_results:
                best_light = self._find_best_model(light_results['results'], 'light')
                comparison_results['best_models'][f'{target_name}_light'] = best_light
                comparison_results['model_rankings'][f'{target_name}_light'] = self._rank_models(light_results['results'])
        
        # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –≤–∞–∂–∫—ñ –º–æ–¥–µ–ª—ñ (—è–∫—â–æ —î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)
        heavy_results_path = Path("data/colab/from_training")
        if heavy_results_path.exists():
            for target_name, heavy_results in training_results['heavy_models'].items():
                results_file = heavy_results_path / f"results_{target_name}.json"
                if results_file.exists():
                    with open(results_file, 'r') as f:
                        heavy_data = json.load(f)
                        best_heavy = self._find_best_model(heavy_data, 'heavy')
                        comparison_results['best_models'][f'{target_name}_heavy'] = best_heavy
                        comparison_results['model_rankings'][f'{target_name}_heavy'] = self._rank_models(heavy_data)
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—ñ —Å–∏–≥–Ω–∞–ª–∏
        comparison_results['final_signals'] = self._generate_final_signals(comparison_results['best_models'])
        
        # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π
        total_models = sum(len(ranking) for ranking in comparison_results['model_rankings'].values())
        comparison_results['metadata']['total_models_compared'] = total_models
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–µ—à
        pickle.dump(comparison_results, open(cache_file, 'wb'))
        
        logger.info(f"[ModelComparator] –ü–æ—Ä—ñ–≤–Ω—è–Ω–æ {total_models} –º–æ–¥–µ–ª–µ–π")
        return comparison_results
    
    def _find_best_model(self, model_results: Dict[str, Dict], model_type: str) -> Dict[str, Any]:
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å —Å–µ—Ä–µ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤"""
        if not model_results:
            return {'model': 'none', 'score': 0, 'metrics': {}}
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ F1 score —è–∫ –æ—Å–Ω–æ–≤–Ω—É –º–µ—Ç—Ä–∏–∫—É
        best_model = None
        best_score = -1
        
        for model_name, metrics in model_results.items():
            score = metrics.get('f1_score', 0)
            if score > best_score:
                best_score = score
                best_model = model_name
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'metrics': model_results[best_model],
                'type': model_type
            }
        
        return {'model': 'none', 'score': 0, 'metrics': {}, 'type': model_type}
    
    def _rank_models(self, model_results: Dict[str, Dict]) -> List[Dict[str, Any]]:
        """–†–∞–Ω–∂—É—î –º–æ–¥–µ–ª—ñ –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é"""
        if not model_results:
            return []
        
        ranked_models = []
        for model_name, metrics in model_results.items():
            ranked_models.append({
                'model': model_name,
                'f1_score': metrics.get('f1_score', 0),
                'accuracy': metrics.get('accuracy', 0),
                'precision': metrics.get('precision', 0),
                'recall': metrics.get('recall', 0)
            })
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ F1 score
        ranked_models.sort(key=lambda x: x['f1_score'], reverse=True)
        return ranked_models
    
    def _generate_final_signals(self, best_models: Dict[str, Dict]) -> Dict[str, Dict]:
        """–ì–µ–Ω–µ—Ä—É—î —Ñ—ñ–Ω–∞–ª—å–Ω—ñ —Ç–æ—Ä–≥–æ–≤—ñ —Å–∏–≥–Ω–∞–ª–∏"""
        signals = {}
        
        for target_key, best_model_info in best_models.items():
            target_name = target_key.replace('_light', '').replace('_heavy', '')
            model_type = 'light' if '_light' in target_key else 'heavy'
            
            # –°–∏–º—É–ª—é—î–º–æ —Å–∏–≥–Ω–∞–ª –Ω–∞ –æ—Å–Ω–æ–≤—ñ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª—ñ
            accuracy = best_model_info['metrics'].get('accuracy', 0.5)
            
            # –ì–µ–Ω–µ—Ä—É—î–º–æ —Å–∏–≥–Ω–∞–ª –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–æ—á–Ω–æ—Å—Ç—ñ
            if accuracy > 0.7:
                signal = 1  # BUY
                confidence = accuracy
            elif accuracy < 0.4:
                signal = -1  # SELL
                confidence = 1 - accuracy
            else:
                signal = 0  # HOLD
                confidence = 0.5
            
            signals[target_name] = {
                'signal': signal,
                'confidence': confidence,
                'recommended_model': best_model_info['model'],
                'model_type': model_type,
                'accuracy': accuracy,
                'f1_score': best_model_info['metrics'].get('f1_score', 0)
            }
        
        return signals
    
    def _is_comparison_fresh(self, data: Dict[str, Any], max_age_hours: int = 6) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å–≤—ñ–∂—ñ"""
        comparison_time = data.get('metadata', {}).get('comparison_time')
        if not comparison_time:
            return False
        
        age = datetime.now() - comparison_time
        return age.total_seconds() < max_age_hours * 3600

class FinalPipeline:
    """–§—ñ–Ω–∞–ª—å–Ω–∏–π —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω"""
    
    def __init__(self, cache_base_dir: str = "data/cache"):
        self.cache_base_dir = Path(cache_base_dir)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≤—Å—ñ –µ—Ç–∞–ø–∏
        self.data_parser = DataParser(str(self.cache_base_dir / "raw"))
        self.data_enricher = DataEnricher(str(self.cache_base_dir / "enriched"))
        self.feature_selector = FeatureSelector(str(self.cache_base_dir / "features"))
        self.model_trainer = ModelTrainer(str(self.cache_base_dir / "models"))
        self.model_comparator = ModelComparator(str(self.cache_base_dir / "comparison"))
        
        logger.info("[FinalPipeline] Initialized all pipeline stages")
    
    def run_complete_pipeline(self, 
                            target_config: Optional[Dict] = None,
                            model_config: Optional[Dict] = None,
                            force_refresh: bool = False) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞—î –ø–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –≤—ñ–¥ –ø–æ—á–∞—Ç–∫—É –¥–æ –∫—ñ–Ω—Ü—è"""
        logger.info("[FinalPipeline] –ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω—É...")
        start_time = datetime.now()
        
        results = {
            'metadata': {
                'start_time': start_time,
                'pipeline_version': 'final_unified',
                'force_refresh': force_refresh
            }
        }
        
        try:
            # –ï—Ç–∞–ø 1: –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–∏—Ö
            logger.info("üîÑ –ï—Ç–∞–ø 1: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö...")
            raw_data = self.data_parser.parse_all_data(force_refresh)
            results['stage_1_collection'] = {
                'status': 'completed',
                'data_types': list(raw_data.keys()),
                'timestamp': datetime.now()
            }
            
            # –ï—Ç–∞–ø 2: –ó–±–∞–≥–∞—á–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
            logger.info("üîÑ –ï—Ç–∞–ø 2: –ó–±–∞–≥–∞—á–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")
            enriched_df, enrichment_metadata = self.data_enricher.enrich_data(raw_data, force_refresh)
            results['stage_2_enrichment'] = {
                'status': 'completed',
                'shape': enriched_df.shape if enriched_df is not None else None,
                'metadata': enrichment_metadata,
                'timestamp': datetime.now()
            }
            
            # –ï—Ç–∞–ø 3: –í–∏–±—ñ—Ä —Ñ—ñ—á
            logger.info("üîÑ –ï—Ç–∞–ø 3: –í–∏–±—ñ—Ä —Ñ—ñ—á...")
            features_dict = self.feature_selector.prepare_features(enriched_df, target_config, force_refresh)
            results['stage_3_features'] = {
                'status': 'completed',
                'targets': list(features_dict['features_by_target'].keys()),
                'metadata': features_dict['metadata'],
                'timestamp': datetime.now()
            }
            
            # –ï—Ç–∞–ø 4: –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
            logger.info("üîÑ –ï—Ç–∞–ø 4: –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π...")
            training_results = self.model_trainer.train_models(features_dict, model_config, force_refresh)
            results['stage_4_training'] = {
                'status': 'completed',
                'light_models': list(training_results['light_models'].keys()),
                'heavy_models': list(training_results['heavy_models'].keys()),
                'metadata': training_results['metadata'],
                'timestamp': datetime.now()
            }
            
            # –ï—Ç–∞–ø 5: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
            logger.info("üîÑ –ï—Ç–∞–ø 5: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π...")
            comparison_results = self.model_comparator.compare_models(training_results, force_refresh)
            results['stage_5_comparison'] = {
                'status': 'completed',
                'best_models': list(comparison_results['best_models'].keys()),
                'final_signals': list(comparison_results['final_signals'].keys()),
                'metadata': comparison_results['metadata'],
                'timestamp': datetime.now()
            }
            
            # –§—ñ–Ω–∞–ª—å–Ω—ñ —Å–∏–≥–Ω–∞–ª–∏
            results['final_signals'] = comparison_results['final_signals']
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ
            end_time = datetime.now()
            duration = end_time - start_time
            results['metadata']['end_time'] = end_time
            results['metadata']['duration'] = str(duration)
            results['metadata']['status'] = 'success'
            
            logger.info(f"[FinalPipeline] ‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration}")
            
        except Exception as e:
            logger.error(f"[FinalPipeline] ‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω—ñ: {e}")
            results['metadata']['status'] = 'failed'
            results['metadata']['error'] = str(e)
            results['metadata']['end_time'] = datetime.now()
            
            raise
        
        return results
    
    def run_stage_only(self, stage: int, **kwargs) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞—î —Ç—ñ–ª—å–∫–∏ –≤–∫–∞–∑–∞–Ω–∏–π –µ—Ç–∞–ø"""
        logger.info(f"[FinalPipeline] –ó–∞–ø—É—Å–∫ —Ç—ñ–ª—å–∫–∏ –µ—Ç–∞–ø—É {stage}...")
        
        if stage == 1:
            raw_data = self.data_parser.parse_all_data(kwargs.get('force_refresh', False))
            return {'status': 'completed', 'data': raw_data}
        
        elif stage == 2:
            # –ü–æ—Ç—Ä—ñ–±–Ω—ñ –¥–∞–Ω—ñ –∑ –µ—Ç–∞–ø—É 1
            raw_data = kwargs.get('raw_data')
            if not raw_data:
                raw_data = self.data_parser.parse_all_data(kwargs.get('force_refresh', False))
            
            enriched_df, metadata = self.data_enricher.enrich_data(raw_data, kwargs.get('force_refresh', False))
            return {'status': 'completed', 'data': enriched_df, 'metadata': metadata}
        
        elif stage == 3:
            # –ü–æ—Ç—Ä—ñ–±–Ω—ñ –¥–∞–Ω—ñ –∑ –µ—Ç–∞–ø—É 2
            enriched_df = kwargs.get('enriched_df')
            if not enriched_df:
                # –ó–∞–ø—É—Å–∫–∞—î–º–æ –µ—Ç–∞–ø–∏ 1-2
                raw_data = self.data_parser.parse_all_data(kwargs.get('force_refresh', False))
                enriched_df, _ = self.data_enricher.enrich_data(raw_data, kwargs.get('force_refresh', False))
            
            features_dict = self.feature_selector.prepare_features(
                enriched_df, 
                kwargs.get('target_config'), 
                kwargs.get('force_refresh', False)
            )
            return {'status': 'completed', 'data': features_dict}
        
        elif stage == 4:
            # –ü–æ—Ç—Ä—ñ–±–Ω—ñ –¥–∞–Ω—ñ –∑ –µ—Ç–∞–ø—É 3
            features_dict = kwargs.get('features_dict')
            if not features_dict:
                # –ó–∞–ø—É—Å–∫–∞—î–º–æ –µ—Ç–∞–ø–∏ 1-3
                raw_data = self.data_parser.parse_all_data(kwargs.get('force_refresh', False))
                enriched_df, _ = self.data_enricher.enrich_data(raw_data, kwargs.get('force_refresh', False))
                features_dict = self.feature_selector.prepare_features(
                    enriched_df, 
                    kwargs.get('target_config'), 
                    kwargs.get('force_refresh', False)
                )
            
            training_results = self.model_trainer.train_models(
                features_dict, 
                kwargs.get('model_config'), 
                kwargs.get('force_refresh', False)
            )
            return {'status': 'completed', 'data': training_results}
        
        elif stage == 5:
            # –ü–æ—Ç—Ä—ñ–±–Ω—ñ –¥–∞–Ω—ñ –∑ –µ—Ç–∞–ø—É 4
            training_results = kwargs.get('training_results')
            if not training_results:
                # –ó–∞–ø—É—Å–∫–∞—î–º–æ –µ—Ç–∞–ø–∏ 1-4
                raw_data = self.data_parser.parse_all_data(kwargs.get('force_refresh', False))
                enriched_df, _ = self.data_enricher.enrich_data(raw_data, kwargs.get('force_refresh', False))
                features_dict = self.feature_selector.prepare_features(
                    enriched_df, 
                    kwargs.get('target_config'), 
                    kwargs.get('force_refresh', False)
                )
                training_results = self.model_trainer.train_models(
                    features_dict, 
                    kwargs.get('model_config'), 
                    kwargs.get('force_refresh', False)
                )
            
            comparison_results = self.model_comparator.compare_models(
                training_results, 
                kwargs.get('force_refresh', False)
            )
            return {'status': 'completed', 'data': comparison_results}
        
        else:
            raise ValueError(f"–ù–µ–≤—ñ–¥–æ–º–∏–π –µ—Ç–∞–ø: {stage}")
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç—É—Å –ø–∞–π–ø–ª–∞–π–Ω—É —Ç–∞ –∫–µ—à—ñ–≤"""
        status = {
            'pipeline_version': 'final_unified',
            'cache_status': {},
            'last_runs': {}
        }
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—Ç–∞—Ç—É—Å –∫–µ—à—ñ–≤
        cache_dirs = ['raw', 'enriched', 'features', 'models', 'comparison']
        for cache_dir in cache_dirs:
            cache_path = self.cache_base_dir / cache_dir
            if cache_path.exists():
                files = list(cache_path.glob("*"))
                status['cache_status'][cache_dir] = {
                    'exists': True,
                    'files_count': len(files),
                    'latest_file': max(files, key=lambda f: f.stat().st_mtime).name if files else None
                }
            else:
                status['cache_status'][cache_dir] = {'exists': False, 'files_count': 0}
        
        return status

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∑—Ä—É—á–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
final_pipeline = FinalPipeline()
