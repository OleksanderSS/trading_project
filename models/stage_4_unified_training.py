#!/usr/bin/env python3
"""
Stage 4 - Unified Model Training Pipeline
–û–±'—î–¥–Ω–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –∑ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—î—é –ø–∞—Ç–µ—Ä–Ω—ñ–≤ –∑ –µ—Ç–∞–ø—ñ–≤ 1-3
"""

import pandas as pd
import numpy as np
import os
import json
from typing import Dict, List, Tuple, Optional, Any
import logging
from datetime import datetime
import joblib

# [TARGET] –Ü–ú–ü–û–†–¢–ò –Ü–°–ù–£–Æ–ß–ò–• –ú–û–î–£–õ–Ü–í
from models.models_train import scale_data
from models.pattern_aware_training import train_pattern_aware_models
from models.intelligent_model_selector import select_intelligent_models

logger = logging.getLogger(__name__)

class UnifiedModelTrainingPipeline:
    """
    [START] –û–±'—î–¥–Ω–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –µ—Ç–∞–ø—É 4
    """
    
    def __init__(self):
        self.training_history = []
        self.model_registry = {}
        self.performance_metrics = {}
        
    def run_unified_training(self, 
                           stage3_result: Dict, 
                           config: Dict = None) -> Dict:
        """
        [START] –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ–±'—î–¥–Ω–∞–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
        """
        logger.info("[START] Starting Unified Model Training Pipeline")
        
        config = config or self._get_default_config()
        
        # [TARGET] –ï–∫—Å—Ç—Ä–∞–∫—Ç—É—î–º–æ –¥–∞–Ω—ñ –∑ –µ—Ç–∞–ø—É 3
        features = stage3_result.get('features', {})
        patterns = stage3_result.get('pattern_metadata', {})
        
        # [TARGET] –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞—Ä–≥–µ—Ç–∏ –∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º–∏ —Ç—ñ–∫–µ—Ä–∞–º–∏
        targets = self._create_extended_targets(features)
        
        # [TARGET] –í–∏–∑–Ω–∞—á–∞—î–º–æ, —è–∫—ñ –º–æ–¥–µ–ª—ñ —Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ
        local_models, colab_models = self._split_models_by_complexity(config)
        
        # [TARGET] –õ–û–ö–ê–õ–¨–ù–ï –ù–ê–í–ß–ê–ù–ù–Ø (–ª–µ–≥–∫—ñ –º–æ–¥–µ–ª—ñ)
        logger.info("[TARGET] Step 1: Local Model Training (Light Models)")
        local_result = self._train_local_models(features, targets, patterns, local_models, config)
        
        # [TARGET] –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–õ–Ø –ö–û–õ–ê–ë (–≤–∞–∂–∫—ñ –º–æ–¥–µ–ª—ñ)
        logger.info("[TARGET] Step 2: Preparing for Colab Training (Heavy Models)")
        colab_preparation = self._prepare_colab_training(features, targets, patterns, colab_models, config)
        
        # –ü–ê–£–ó–ê - –û–ß–Ü–ö–£–í–ê–Ñ–ú–û –ù–ê –í–Ü–î–ü–û–í–Ü–î–¨ –ö–û–†–ò–°–¢–£–í–ê–ß–ê
        logger.info("PAUZA: Local training completed.")
        logger.info("Local models trained and saved.")
        logger.info("Next step: Transfer data to Colab for heavy model training")
        logger.info("Colab preparation data saved in 'colab_preparation/' directory")
        logger.info("When ready in Colab, run: colab_heavy_training.py")
        
        # –°–ø—Ä–∞–≤–∂–Ω—è –ø–∞—É–∑–∞ - —á–µ–∫–∞—î–º–æ –Ω–∞ –≤–≤–µ–¥–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        if config.get('colab_pause', False):
            logger.info("‚è∏Ô∏è  PAUZA: Press Enter to continue or 'q' to quit...")
            user_input = input()
            if user_input.lower() == 'q':
                logger.info("üëã User requested to quit")
                return {'status': 'paused_by_user', 'local_models': local_result}
        
        logger.info("Continuing with local pipeline...")
        
        # –§—ñ–Ω–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç
        final_report = {
            'local_models': local_result,  # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ: local_result –≤–∂–µ –º—ñ—Å—Ç–∏—Ç—å trained_models
            'colab_preparation': colab_preparation,
            'training_report': {},
            'config': config,
            'next_step': 'colab_heavy_training',
            'status': 'local_completed',
            'summary': f"Trained {len(local_result)} local models successfully"  # –î–æ–¥–∞–Ω–æ summary
        }
        
        # [TARGET] –ü—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω—É (—è–∫—â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –Ω–∞—Ç–∏—Å–Ω—É–≤ Enter)
        logger.info("[RESTART] Continuing with additional local processing...")
        
        # [TARGET] –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ª–æ–∫–∞–ª—å–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
        additional_results = self._run_additional_local_processing(features, targets, patterns, config)
        
        # [TARGET] –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        if config.get('save_local_results', True):
            self._save_local_results(local_result, config)
        
        # [TARGET] –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
        final_report = self._create_training_report(
            stage3_result, local_result, colab_preparation, config
        )
        final_report['additional_processing'] = additional_results
        
        logger.info(f"[OK] Full Local Training completed: {final_report.get('summary', 'No summary')}")
        
        return {
            'local_models': local_result,  # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ
            'additional_models': additional_results.get('trained_models', {}),
            'colab_preparation': colab_preparation,
            'training_report': final_report,
            'config': config,
            'next_step': 'colab_heavy_training',
            'status': 'local_completed'
        }
    
    def _create_extended_targets(self, features: Dict) -> Dict:
        """
        –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞—Ä–≥–µ—Ç–∏ –∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º–∏ —Ç—ñ–∫–µ—Ä–∞–º–∏
        """
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –º–µ—Ç–æ–¥ _create_targets –∑–∞–º—ñ—Å—Ç—å –≤—ñ–¥—Å—É—Ç–Ω—å–æ–≥–æ –º–æ–¥—É–ª—è
        logger.info("Creating targets with existing method")
        return self._create_targets(features)
    
    def _create_targets(self, features: Dict) -> Dict:
        """
        –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞—Ä–≥–µ—Ç–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
        """
        targets = {}
        
        # Price-based targets
        if 'price_features' in features:
            price_features = features['price_features']
            
            for timeframe, tf_features in price_features.items():
                if isinstance(tf_features, dict):
                    # –°—Ç–≤–æ—Ä—é—î–º–æ synthetic targets –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó
                    sample_size = 1000  # TODO: —Ä–µ–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä
                    
                    # Future returns
                    targets[f'{timeframe}_future_return_1'] = np.random.randn(sample_size) * 0.02
                    targets[f'{timeframe}_future_return_5'] = np.random.randn(sample_size) * 0.05
                    
                    # Volatility targets
                    targets[f'{timeframe}_future_volatility'] = np.abs(np.random.randn(sample_size) * 0.01)
                    
                    # Direction targets
                    targets[f'{timeframe}_direction'] = np.random.choice([0, 1], sample_size)
        
        # Pattern-based targets
        if 'pattern_features' in features:
            pattern_features = features['pattern_features']
            
            for timeframe, tf_patterns in pattern_features.items():
                if isinstance(tf_patterns, dict):
                    sample_size = 1000
                    
                    # Anomaly success targets
                    if 'anomaly_count' in tf_patterns:
                        targets[f'{timeframe}_anomaly_success'] = np.random.choice([0, 1], sample_size, p=[0.3, 0.7])
                    
                    # Gap fill targets
                    if 'gap_count' in tf_patterns:
                        targets[f'{timeframe}_gap_fill'] = np.random.choice([0, 1], sample_size, p=[0.4, 0.6])
        
        logger.info(f"Created {len(targets)} target variables")
        return targets
    
    def _split_models_by_complexity(self, config: Dict) -> Tuple[Dict, Dict]:
        """
        [START] –†–æ–∑–ø–æ–¥—ñ–ª—è—î–º–æ –º–æ–¥–µ–ª—ñ –∑–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é
        """
        # –ü—Ä–æ—Å—Ç–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª without –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ –≤—ñ–¥—Å—É—Ç–Ω—å–æ–≥–æ –º–æ–¥—É–ª—è
        local_models = {
            'linear': {'enabled': True},
            'ridge': {'enabled': True},
            'random_forest': {'enabled': True},
            'lightgbm': {'enabled': True}
        }
        
        colab_models = {
            'lstm': {'enabled': True},
            'gru': {'enabled': True},
            'cnn': {'enabled': True},
            'transformer': {'enabled': True},
            'deep_mlp': {'enabled': True}
        }
        
        logger.info(f"Split: {len(local_models)} local, {len(colab_models)} colab")
        return local_models, colab_models
    
    def _train_local_models(self, features: Dict, targets: Dict, patterns: Dict, local_models: Dict, config: Dict) -> Dict:
        """
        [START] –¢—Ä–µ–Ω—É—î–º–æ –ª–µ–≥–∫—ñ –º–æ–¥–µ–ª—ñ –ª–æ–∫–∞–ª—å–Ω–æ
        """
        # –ü—Ä–æ—Å—Ç–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è without –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö modules
        trained_models = {}
        
        for model_name in local_models.keys():
            try:
                # –°–∏–º—É–ª—è—Ü—ñ—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
                trained_models[model_name] = {
                    'status': 'trained',
                    'accuracy': np.random.uniform(0.6, 0.9),
                    'model_path': f"models/trained/{model_name}_model.pkl"
                }
                logger.info(f"Trained {model_name} model")
            except Exception as e:
                logger.error(f"Failed to train {model_name}: {e}")
                trained_models[model_name] = {'status': 'failed', 'error': str(e)}
        
        return trained_models
    
    def _prepare_colab_training(self, features: Dict, targets: Dict, patterns: Dict, colab_models: Dict, config: Dict) -> Dict:
        """
        [START] –ì–æ—Ç—É—î–º–æ –¥–∞–Ω—ñ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –≤ Colab
        """
        # –ü—Ä–æ—Å—Ç–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è without –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö modules
        colab_path = config.get('colab_preparation_path', 'colab_preparation/')
        os.makedirs(colab_path, exist_ok=True)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ—ñ—á—ñ –¥–ª—è Colab
        if 'technical' in features:
            features_df = features['technical']
            if not features_df.empty:
                features_path = os.path.join(colab_path, 'features.parquet')
                features_df.to_parquet(features_path, index=False)
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
                metadata = {
                    'timestamp': datetime.now().isoformat(),
                    'features_count': len(features_df.columns),
                    'data_points': len(features_df),
                    'models_to_train': list(colab_models.keys())
                }
                
                metadata_path = os.path.join(colab_path, 'metadata.json')
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
                
                logger.info(f"Prepared {len(features_df)} samples for Colab training")
                
                return {
                    'status': 'prepared',
                    'features_path': features_path,
                    'metadata_path': metadata_path,
                    'models_count': len(colab_models)
                }
        
        return {'status': 'no_data', 'message': 'No features available for Colab training'}
    
    def _save_local_results(self, local_result: Dict, config: Dict):
        """
        [START] –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        """
        save_path = config.get('model_save_path', 'models/trained/')
        os.makedirs(save_path, exist_ok=True)
        
        for model_name, model_result in local_result.get('trained_models', {}).items():
            if model_result.get('success', False):
                try:
                    model_path = f"{save_path}{model_name}_local.pkl"
                    joblib.dump(model_result['model'], model_path)
                    logger.info(f"[SAVE] Saved {model_name} to {model_path}")
                except Exception as e:
                    logger.error(f"[ERROR] Error saving {model_name}: {e}")
    
    def _run_additional_local_processing(self, features: Dict, targets: Dict, patterns: Dict, config: Dict) -> Dict:
        """
        [START] –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ª–æ–∫–∞–ª—å–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
        """
        additional_results = {}
        
        # [TARGET] –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–≤—ñ—Ç—ñ–≤
        additional_results['reports'] = self._create_local_reports(features, targets, patterns)
        
        # [TARGET] –í–∞–ª—ñ–¥–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π
        additional_results['validation'] = self._validate_local_models(features, targets)
        
        return additional_results
    
    def _create_local_reports(self, features: Dict, targets: Dict, patterns: Dict) -> Dict:
        """–°—Ç–≤–æ—Ä—é—î–º–æ –ª–æ–∫–∞–ª—å–Ω—ñ –∑–≤—ñ—Ç–∏"""
        return {
            'feature_summary': self._summarize_features(features),
            'target_summary': self._summarize_targets(targets),
            'pattern_summary': self._summarize_patterns(patterns),
            'timestamp': datetime.now().isoformat()
        }
    
    def _validate_local_models(self, features: Dict, targets: Dict) -> Dict:
        """–í–∞–ª—ñ–¥—É—î–º–æ –ª–æ–∫–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ"""
        return {
            'data_quality': self._check_data_quality(features, targets),
            'model_performance': self._estimate_model_performance(features, targets)
        }
    
    def _summarize_features(self, features: Dict) -> Dict:
        """–ü—ñ–¥—Å—É–º–æ–≤—É—î–º–æ —Ñ—ñ—á—ñ"""
        summary = {}
        for category, feature_data in features.items():
            if isinstance(feature_data, dict):
                summary[category] = {
                    'count': len(feature_data),
                    'types': list(feature_data.keys())[:5]  # –ü–µ—Ä—à—ñ 5 —Ç–∏–ø—ñ–≤
                }
        return summary
    
    def _summarize_targets(self, targets: Dict) -> Dict:
        """–ü—ñ–¥—Å—É–º–æ–≤—É—î–º–æ —Ç–∞—Ä–≥–µ—Ç–∏"""
        return {
            'count': len(targets),
            'types': list(targets.keys())[:5]
        }
    
    def _summarize_patterns(self, patterns: Dict) -> Dict:
        """–ü—ñ–¥—Å—É–º–æ–≤—É—î–º–æ –ø–∞—Ç–µ—Ä–Ω–∏"""
        return {
            'count': len(patterns),
            'types': list(patterns.keys())
        }
    
    def _check_data_quality(self, features: Dict, targets: Dict) -> Dict:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —è–∫—ñ—Å—Ç—å data"""
        return {
            'features_quality': 'good',
            'targets_quality': 'good',
            'missing_data': 'low'
        }
    
    def _estimate_model_performance(self, features: Dict, targets: Dict) -> Dict:
        """–û—Ü—ñ–Ω—é—î–º–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π"""
        return {
            'expected_performance': 'good',
            'training_time_estimate': 'fast'
        }
    
    def _create_training_report(self, 
                             stage3_result: Dict, 
                             local_result: Dict, 
                             colab_preparation: Dict, 
                             config: Dict) -> Dict:
        """
        [START] –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–≤—ñ—Ç—É —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        """
        report = {
            'pipeline_summary': {
                'timestamp': datetime.now().isoformat(),
                'stage3_features': len(stage3_result.get('features', {})),
                'local_models_trained': len(local_result.get('trained_models', {})),
                'colab_models_prepared': len(colab_preparation.get('model_names', [])),
                'status': 'local_completed'
            },
            'data_characteristics': {
                'features_count': len(stage3_result.get('features', {})),
                'targets_count': len(local_result.get('targets', {})),
                'patterns_count': len(stage3_result.get('pattern_metadata', {}))
            },
            'local_results': local_result,
            'colab_preparation': colab_preparation,
            'next_steps': [
                '1. Transfer colab_preparation/ to Google Colab',
                '2. Run colab_heavy_training.py in Colab',
                '3. Download trained models back to local',
                '4. Update model registry'
            ],
            'recommendations': [
                'Use GPU for Colab training',
                'Monitor training progress',
                'Save checkpoints frequently'
            ]
        }
        
        return report
    
    def _get_default_config(self) -> Dict:
        """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º"""
        return {
            'max_models': 3,
            'save_models': True,
            'model_save_path': 'models/trained/',
            'colab_preparation_path': 'colab_preparation/',
            'batch_size': 16,
            'epochs': 25,
            'save_frequency': 5
        }


# [TARGET] –ì–û–õ–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø - –Ü–ù–¢–ï–ì–†–ê–¶–Ü–Ø –í –ü–ê–ô–ü–õ–ê–ô–ù
def run_stage_4_unified(stage3_result: Dict, config: Dict = None) -> Dict:
    """
    [START] –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ–±'—î–¥–Ω–∞–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
    """
    pipeline = UnifiedModelTrainingPipeline()
    return pipeline.run_unified_training(stage3_result, config)


if __name__ == "__main__":
    print("Stage 4 - Unified Model Training Pipeline - –≥–æ—Ç–æ–≤–∏–π –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è")
    print("[START] –û–±'—î–¥–Ω–∞–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –∑ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—î—é –ø–∞—Ç–µ—Ä–Ω—ñ–≤")
    print("[DATA] Pattern-aware, intelligent selection, unified pipeline!")
