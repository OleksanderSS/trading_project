#!/usr/bin/env python3
"""
Hedge Fund Analysis Module
–ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ö–µ–¥–∂ —Ñ–æ–Ω–¥—ñ–≤: –¥–µ—Ç–µ–∫—Ü—ñ—è —Å—Ç–∏–ª—é, –µ–∫—Å–ø–æ–∑–∏—Ü—ñ—ó, —Ä–∏–∑–∏–∫–∏
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging
import warnings
warnings.filterwarnings('ignore')

from models.fama_french_factors import get_fama_french_factors, calculate_factor_exposures
from utils.economic_context_mapper import get_economic_context

logger = logging.getLogger(__name__)


class HedgeFundAnalyzer:
    """
    –ê–Ω–∞–ª—ñ–∑ —Ö–µ–¥–∂ —Ñ–æ–Ω–¥—ñ–≤ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –Ω–∞–≤–∏—á–æ–∫ –º–µ–Ω–µ–¥–∂–µ—Ä—ñ–≤ —Ç–∞ —Å—Ç—ñ–π–∫–æ—Å—Ç—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π
    """
    
    def __init__(self):
        self.logger = logging.getLogger("HedgeFundAnalyzer")
        
        # –§–∞–∫—Ç–æ—Ä–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        self.factor_models = {
            'carhart': ['MKT', 'SMB', 'HML', 'UMD'],  # Carhart 4-factor
            'french_5': ['MKT', 'SMB', 'HML', 'RMW', 'CMA'],  # Fama-French 5-factor
            'french_6': ['MKT', 'SMB', 'HML', 'UMD', 'RMW', 'CMA']  # Full 6-factor
        }
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü—ñ—ó —Å—Ç–∏–ª—é
        self.style_thresholds = {
            'market_exposure': 0.7,      # >0.7 = high beta
            'size_tilt': 0.3,           # >0.3 = small cap bias
            'value_tilt': 0.3,          # >0.3 = value bias
            'momentum_tilt': 0.3,       # >0.3 = momentum bias
            'alpha_significance': 0.05   # p-value < 0.05 = significant alpha
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∏–∑–∏–∫—É
        self.risk_metrics = [
            'volatility', 'sharpe', 'sortino', 'max_drawdown',
            'var_95', 'var_99', 'cvar_95', 'cvar_99',
            'calmar_ratio', 'tail_ratio', 'skewness', 'kurtosis'
        ]
        
        self.logger.info("HedgeFundAnalyzer initialized")
    
    def calculate_performance_metrics(self, returns: pd.Series, 
                                   benchmark_returns: pd.Series = None) -> Dict[str, float]:
        """
        –†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –ø–æ–≤–Ω–∏–π –Ω–∞–±—ñ—Ä –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        
        Args:
            returns: –î–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å —Ñ–æ–Ω–¥—É
            benchmark_returns: –ë–µ–Ω—á–º–∞—Ä–∫ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            Dict: –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        """
        try:
            metrics = {}
            
            # –ë–∞–∑–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
            metrics['total_return'] = (1 + returns).prod() - 1
            metrics['annual_return'] = returns.mean() * 252
            metrics['annual_volatility'] = returns.std() * np.sqrt(252)
            
            # Risk-adjusted metrics
            risk_free_rate = 0.02  # 2% —Ä—ñ—á–Ω–∞
            
            # Sharpe Ratio
            excess_returns = returns - risk_free_rate / 252
            metrics['sharpe_ratio'] = excess_returns.mean() / excess_returns.std() * np.sqrt(252)
            
            # Sortino Ratio (—Ç—ñ–ª—å–∫–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å)
            downside_returns = returns[returns < 0]
            if len(downside_returns) > 0:
                downside_vol = downside_returns.std() * np.sqrt(252)
                metrics['sortino_ratio'] = excess_returns.mean() / downside_vol
            else:
                metrics['sortino_ratio'] = np.inf
            
            # Maximum Drawdown
            cumulative = (1 + returns).cumprod()
            running_max = cumulative.expanding().max()
            drawdown = (cumulative - running_max) / running_max
            metrics['max_drawdown'] = drawdown.min()
            
            # Calmar Ratio
            if metrics['max_drawdown'] != 0:
                metrics['calmar_ratio'] = metrics['annual_return'] / abs(metrics['max_drawdown'])
            else:
                metrics['calmar_ratio'] = np.inf
            
            # VaR —ñ CVaR
            metrics['var_95'] = returns.quantile(0.05)
            metrics['var_99'] = returns.quantile(0.01)
            metrics['cvar_95'] = returns[returns <= metrics['var_95']].mean()
            metrics['cvar_99'] = returns[returns <= metrics['var_99']].mean()
            
            # Tail Ratio
            tail_95 = returns.quantile(0.95)
            tail_05 = returns.quantile(0.05)
            metrics['tail_ratio'] = abs(tail_95) / abs(tail_05) if tail_05 != 0 else np.inf
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–æ–∑–ø–æ–¥—ñ–ª—É
            metrics['skewness'] = returns.skew()
            metrics['kurtosis'] = returns.kurtosis()
            
            # Win Rate —ñ Hit Ratio
            metrics['win_rate'] = (returns > 0).sum() / len(returns)
            metrics['hit_ratio'] = metrics['win_rate']  # –¢–µ –∂ —Å–∞–º–µ
            
            # Information Ratio (—è–∫—â–æ —î –±–µ–Ω—á–º–∞—Ä–∫)
            if benchmark_returns is not None:
                active_returns = returns - benchmark_returns
                if active_returns.std() != 0:
                    metrics['information_ratio'] = active_returns.mean() / active_returns.std() * np.sqrt(252)
                else:
                    metrics['information_ratio'] = 0
                metrics['tracking_error'] = active_returns.std() * np.sqrt(252)
            else:
                metrics['information_ratio'] = 0
                metrics['tracking_error'] = 0
            
            self.logger.info(f"Performance metrics calculated: Sharpe={metrics['sharpe_ratio']:.2f}")
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"Error calculating performance metrics: {e}")
            return {}
    
    def calculate_factor_exposures(self, fund_returns: pd.Series, 
                                 factor_model: str = 'french_6') -> Dict[str, any]:
        """
        –†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ —Ñ–∞–∫—Ç–æ—Ä–Ω—ñ –µ–∫—Å–ø–æ–∑–∏—Ü—ñ—ó —Ñ–æ–Ω–¥—É
        
        Args:
            fund_returns: –î–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å —Ñ–æ–Ω–¥—É
            factor_model: –ú–æ–¥–µ–ª—å —Ñ–∞–∫—Ç–æ—Ä—ñ–≤
            
        Returns:
            Dict: –§–∞–∫—Ç–æ—Ä–Ω—ñ –µ–∫—Å–ø–æ–∑–∏—Ü—ñ—ó —Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ñ–∞–∫—Ç–æ—Ä–∏
            factors = get_fama_french_factors()
            
            if factors.empty:
                return {}
            
            # –í–∏–±–∏—Ä–∞—î–º–æ —Ñ–∞–∫—Ç–æ—Ä–∏ –º–æ–¥–µ–ª—ñ
            model_factors = self.factor_models.get(factor_model, self.factor_models['french_6'])
            available_factors = [f for f in model_factors if f in factors.columns]
            
            if not available_factors:
                return {}
            
            factor_data = factors[available_factors]
            
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –µ–∫—Å–ø–æ–∑–∏—Ü—ñ—ó
            exposures = calculate_factor_exposures(fund_returns, factor_data)
            
            # –î–æ–¥–∞—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–Ω–∞—á—É—â–æ—Å—Ç—ñ
            if exposures:
                # T-statistics –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–Ω–∞—á—É—â–æ—Å—Ç—ñ
                t_stats = self._calculate_t_statistics(fund_returns, factor_data)
                
                # P-values
                p_values = {factor: 2 * (1 - stats.t.cdf(abs(t), len(fund_returns) - len(factor_data.columns) - 1))
                           for factor, t in t_stats.items()}
                
                # Significant factors
                significant_factors = {f: exposures[f] for f in exposures 
                                     if f in p_values and p_values[f] < self.style_thresholds['alpha_significance']}
                
                result = {
                    'exposures': exposures,
                    't_statistics': t_stats,
                    'p_values': p_values,
                    'significant_factors': significant_factors,
                    'r_squared': exposures.get('r_squared', 0),
                    'model_type': factor_model
                }
                
                self.logger.info(f"Factor exposures calculated: R¬≤={result['r_squared']:.3f}")
                
                return result
            
            return {}
            
        except Exception as e:
            self.logger.error(f"Error calculating factor exposures: {e}")
            return {}
    
    def detect_style_drift(self, current_exposures: Dict[str, float], 
                          historical_exposures: List[Dict[str, float]]) -> Dict[str, any]:
        """
        –î–µ—Ç–µ–∫—Ü—ñ—è –¥—Ä–∏—Ñ—Ç—É —Å—Ç–∏–ª—é —Ñ–æ–Ω–¥—É
        
        Args:
            current_exposures: –ü–æ—Ç–æ—á–Ω—ñ –µ–∫—Å–ø–æ–∑–∏—Ü—ñ—ó
            historical_exposures: –Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –µ–∫—Å–ø–æ–∑–∏—Ü—ñ—ó
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–µ—Ç–µ–∫—Ü—ñ—ó –¥—Ä–∏—Ñ—Ç—É
        """
        try:
            if not historical_exposures:
                return {'drift_detected': False, 'reason': 'No historical data'}
            
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ —ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –µ–∫—Å–ø–æ–∑–∏—Ü—ñ—ó
            avg_exposures = {}
            for factor in current_exposures:
                if factor != 'alpha' and factor != 'r_squared':
                    factor_values = [h.get(factor, 0) for h in historical_exposures if factor in h]
                    if factor_values:
                        avg_exposures[factor] = np.mean(factor_values)
            
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Ä—ñ–∑–Ω–∏—Ü—é
            drift_scores = {}
            significant_drifts = {}
            
            for factor, current_exp in current_exposures.items():
                if factor in avg_exposures and factor != 'alpha' and factor != 'r_squared':
                    avg_exp = avg_exposures[factor]
                    diff = abs(current_exp - avg_exp)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∞ –∑–Ω–∞—á—É—â—ñ—Å—Ç—å —Ä—ñ–∑–Ω–∏—Ü—ñ
                    historical_values = [h.get(factor, 0) for h in historical_exposures if factor in h]
                    if len(historical_values) > 1:
                        std_dev = np.std(historical_values)
                        z_score = diff / std_dev if std_dev > 0 else 0
                        
                        drift_scores[factor] = {
                            'difference': diff,
                            'z_score': z_score,
                            'significant': z_score > 2.0  # 95% confidence
                        }
                        
                        if drift_scores[factor]['significant']:
                            significant_drifts[factor] = drift_scores[factor]
            
            # –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –¥—Ä–∏—Ñ—Ç—É
            drift_detected = len(significant_drifts) > 0
            
            # –ê–Ω–∞–ª—ñ–∑ —Ç–∏–ø—É –¥—Ä–∏—Ñ—Ç—É
            drift_analysis = self._analyze_drift_type(significant_drifts, current_exposures)
            
            result = {
                'drift_detected': drift_detected,
                'drift_scores': drift_scores,
                'significant_drifts': significant_drifts,
                'drift_analysis': drift_analysis,
                'current_exposures': current_exposures,
                'historical_average': avg_exposures,
                'drift_severity': len(significant_drifts) / len(current_exposures) if current_exposures else 0
            }
            
            self.logger.info(f"Style drift analysis: detected={drift_detected}, severe_factors={len(significant_drifts)}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error detecting style drift: {e}")
            return {'drift_detected': False, 'error': str(e)}
    
    def analyze_manager_skill(self, fund_returns: pd.Series, 
                            benchmark_returns: pd.Series = None) -> Dict[str, any]:
        """
        –ê–Ω–∞–ª—ñ–∑ –Ω–∞–≤–∏—á–æ–∫ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ (alpha —Ç–∞ –Ω–∞–≤–∏—á–∫–∏)
        
        Args:
            fund_returns: –î–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å —Ñ–æ–Ω–¥—É
            benchmark_returns: –ë–µ–Ω—á–º–∞—Ä–∫
            
        Returns:
            Dict: –ê–Ω–∞–ª—ñ–∑ –Ω–∞–≤–∏—á–æ–∫ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
        """
        try:
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            performance = self.calculate_performance_metrics(fund_returns, benchmark_returns)
            
            # –§–∞–∫—Ç–æ—Ä–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
            factor_analysis = self.calculate_factor_exposures(fund_returns)
            
            # Alpha –∞–Ω–∞–ª—ñ–∑
            alpha = factor_analysis.get('exposures', {}).get('alpha', 0)
            alpha_p_value = factor_analysis.get('p_values', {}).get('alpha', 1.0)
            
            # –ù–∞–≤–∏—á–∫–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
            manager_skill = {
                'has_alpha': alpha > 0 and alpha_p_value < self.style_thresholds['alpha_significance'],
                'alpha_annualized': alpha * 252,
                'alpha_significance': alpha_p_value,
                'risk_adjusted_alpha': alpha / fund_returns.std() * np.sqrt(252) if fund_returns.std() > 0 else 0,
                'consistency_score': self._calculate_consistency_score(fund_returns),
                'skill_score': 0  # –†–æ–∑—Ä–∞—Ö—É—î–º–æ –Ω–∏–∂—á–µ
            }
            
            # –ó–∞–≥–∞–ª—å–Ω–∏–π —Å–∫–æ—Ä –Ω–∞–≤–∏—á–æ–∫
            skill_components = []
            
            # Alpha component
            if manager_skill['has_alpha']:
                skill_components.append(min(abs(manager_skill['alpha_annualized']) / 0.05, 1.0))  # 5% alpha = 1.0
            
            # Consistency component
            skill_components.append(manager_skill['consistency_score'])
            
            # Risk-adjusted performance
            if performance['sharpe_ratio'] > 1.0:
                skill_components.append(min(performance['sharpe_ratio'] / 2.0, 1.0))  # Sharpe 2.0 = 1.0
            
            # Downside protection
            if performance['sortino_ratio'] > 1.0:
                skill_components.append(min(performance['sortino_ratio'] / 2.0, 1.0))
            
            # –ó–∞–≥–∞–ª—å–Ω–∏–π —Å–∫–æ—Ä –Ω–∞–≤–∏—á–æ–∫
            if skill_components:
                manager_skill['skill_score'] = np.mean(skill_components)
            else:
                manager_skill['skill_score'] = 0
            
            # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –Ω–∞–≤–∏—á–æ–∫
            if manager_skill['skill_score'] >= 0.8:
                skill_level = 'Exceptional'
            elif manager_skill['skill_score'] >= 0.6:
                skill_level = 'Excellent'
            elif manager_skill['skill_score'] >= 0.4:
                skill_level = 'Good'
            elif manager_skill['skill_score'] >= 0.2:
                skill_level = 'Average'
            else:
                skill_level = 'Poor'
            
            result = {
                'performance_metrics': performance,
                'factor_analysis': factor_analysis,
                'manager_skill': manager_skill,
                'skill_level': skill_level,
                'recommendation': self._generate_skill_recommendation(manager_skill, performance)
            }
            
            self.logger.info(f"Manager skill analysis: level={skill_level}, score={manager_skill['skill_score']:.2f}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error analyzing manager skill: {e}")
            return {}
    
    def _calculate_t_statistics(self, returns: pd.Series, factors: pd.DataFrame) -> Dict[str, float]:
        """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ t-statistics –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–∏—Ö –µ–∫—Å–ø–æ–∑–∏—Ü—ñ–π"""
        try:
            import statsmodels.api as sm
            
            # –û–±'—î–¥–Ω—É—î–º–æ –¥–∞–Ω—ñ
            combined_data = pd.concat([returns, factors], axis=1).dropna()
            
            if len(combined_data) < 30:
                return {}
            
            X = sm.add_constant(combined_data.iloc[:, 1:])
            y = combined_data.iloc[:, 0]
            
            model = sm.OLS(y, X).fit()
            
            # T-statistics
            t_stats = {}
            for i, factor in enumerate(factors.columns):
                if i + 1 < len(model.tvalues):
                    t_stats[factor] = model.tvalues[i + 1]
            
            return t_stats
            
        except Exception as e:
            self.logger.error(f"Error calculating t-statistics: {e}")
            return {}
    
    def _analyze_drift_type(self, significant_drifts: Dict, current_exposures: Dict) -> Dict[str, any]:
        """–ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∏–ø –¥—Ä–∏—Ñ—Ç—É —Å—Ç–∏–ª—é"""
        drift_type = {
            'market_drift': False,
            'size_drift': False,
            'value_drift': False,
            'momentum_drift': False,
            'quality_drift': False,
            'overall_assessment': 'No significant drift'
        }
        
        for factor in significant_drifts:
            if factor == 'MKT':
                drift_type['market_drift'] = True
            elif factor == 'SMB':
                drift_type['size_drift'] = True
            elif factor == 'HML':
                drift_type['value_drift'] = True
            elif factor == 'UMD':
                drift_type['momentum_drift'] = True
            elif factor in ['RMW', 'CMA']:
                drift_type['quality_drift'] = True
        
        # –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
        drift_count = sum([drift_type[k] for k in drift_type if k != 'overall_assessment'])
        
        if drift_count == 0:
            drift_type['overall_assessment'] = 'No significant drift'
        elif drift_count == 1:
            drift_type['overall_assessment'] = 'Minor style drift detected'
        elif drift_count == 2:
            drift_type['overall_assessment'] = 'Moderate style drift detected'
        else:
            drift_type['overall_assessment'] = 'Significant style drift detected'
        
        return drift_type
    
    def _calculate_consistency_score(self, returns: pd.Series) -> float:
        """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ —Å–∫–æ—Ä –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ"""
        try:
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –¥–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å –ø–æ —Ä–æ–∫–∞—Ö
            if len(returns) > 252:
                yearly_returns = returns.resample('Y').apply(lambda x: (1 + x).prod() - 1)
                
                # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ % –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö —Ä–æ–∫—ñ–≤
                positive_years = (yearly_returns > 0).sum()
                consistency = positive_years / len(yearly_returns)
                
                return consistency
            else:
                # –Ø–∫—â–æ –º–µ–Ω—à–µ —Ä–æ–∫—É, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º—ñ—Å—è—á–Ω—ñ –¥–∞–Ω—ñ
                monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)
                positive_months = (monthly_returns > 0).sum()
                consistency = positive_months / len(monthly_returns)
                
                return consistency
                
        except Exception as e:
            self.logger.error(f"Error calculating consistency score: {e}")
            return 0.5  # Default middle value
    
    def _generate_skill_recommendation(self, skill: Dict, performance: Dict) -> str:
        """–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—é based –Ω–∞ –Ω–∞–≤–∏—á–∫–∞—Ö –º–µ–Ω–µ–¥–∂–µ—Ä–∞"""
        try:
            if skill['skill_score'] >= 0.8:
                return "Exceptional manager with consistent alpha generation. Recommend allocation."
            elif skill['skill_score'] >= 0.6:
                return "Excellent manager with good risk-adjusted returns. Consider allocation."
            elif skill['skill_score'] >= 0.4:
                return "Good manager with moderate skill. Monitor closely."
            elif skill['skill_score'] >= 0.2:
                return "Average manager with limited alpha. Consider smaller allocation."
            else:
                return "Poor manager with negative or insignificant alpha. Avoid allocation."
                
        except Exception as e:
            self.logger.error(f"Error generating recommendation: {e}")
            return "Unable to generate recommendation"


# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫–∑–µ–º–ø–ª—è—Ä
hedge_fund_analyzer = HedgeFundAnalyzer()


def analyze_hedge_fund(fund_returns: pd.Series, benchmark_returns: pd.Series = None) -> Dict[str, any]:
    """–ê–Ω–∞–ª—ñ–∑ —Ö–µ–¥–∂ —Ñ–æ–Ω–¥—É"""
    return hedge_fund_analyzer.analyze_manager_skill(fund_returns, benchmark_returns)


def detect_style_drift(current_exposures: Dict[str, float], 
                      historical_exposures: List[Dict[str, float]]) -> Dict[str, any]:
    """–î–µ—Ç–µ–∫—Ü—ñ—è –¥—Ä–∏—Ñ—Ç—É —Å—Ç–∏–ª—é"""
    return hedge_fund_analyzer.detect_style_drift(current_exposures, historical_exposures)


if __name__ == "__main__":
    # –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
    logging.basicConfig(level=logging.INFO)
    
    print("üè¶ Hedge Fund Analyzer Test")
    print("="*50)
    
    # –°–∏–º—É–ª—è—Ü—ñ—è data —Ö–µ–¥–∂ —Ñ–æ–Ω–¥—É
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', '2024-01-01', freq='D')
    
    # –°–∏–º—É–ª—è—Ü—ñ—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ —Ñ–æ–Ω–¥—É
    fund_returns = pd.Series(
        np.random.normal(0.0008, 0.012, len(dates)),  # 20% annual, 12% vol
        index=dates
    )
    
    # –ë–µ–Ω—á–º–∞—Ä–∫ (S&P 500)
    benchmark_returns = pd.Series(
        np.random.normal(0.0006, 0.010, len(dates)),  # 15% annual, 10% vol
        index=dates
    )
    
    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ñ–æ–Ω–¥
    analysis = analyze_hedge_fund(fund_returns, benchmark_returns)
    
    if analysis:
        print(f"[DATA] Manager Skill Analysis:")
        print(f"   Skill Level: {analysis['skill_level']}")
        print(f"   Skill Score: {analysis['manager_skill']['skill_score']:.2f}")
        print(f"   Alpha (annual): {analysis['manager_skill']['alpha_annualized']:.2%}")
        print(f"   Sharpe Ratio: {analysis['performance_metrics']['sharpe_ratio']:.2f}")
        print(f"   Max Drawdown: {analysis['performance_metrics']['max_drawdown']:.2%}")
        print(f"   Recommendation: {analysis['recommendation']}")
        
        # –§–∞–∫—Ç–æ—Ä–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
        factor_analysis = analysis.get('factor_analysis', {})
        if factor_analysis:
            print(f"\n[TARGET] Factor Exposures:")
            exposures = factor_analysis.get('exposures', {})
            for factor, exposure in exposures.items():
                if factor not in ['alpha', 'r_squared']:
                    print(f"   {factor}: {exposure:.3f}")
            print(f"   Alpha: {exposures.get('alpha', 0):.4f}")
            print(f"   R¬≤: {exposures.get('r_squared', 0):.3f}")
        
        print(f"\n[OK] Hedge Fund Analysis working correctly!")
    else:
        print(f"[ERROR] Analysis failed")
